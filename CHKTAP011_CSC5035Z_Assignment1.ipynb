{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGzT6u-0Qyyc"
      },
      "source": [
        "# CSC5035Z Natural Language Processing\n",
        "# Assignment 1: Naive Bayes for Test Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ismKibXLQyyg"
      },
      "source": [
        "**Author: Tapera Chikumbu**\n",
        "\n",
        "**Introduction**\n",
        "Implementing Naive Bayes classifier for sentiment analysis of afrisent data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDmJce36Qyyh"
      },
      "source": [
        "# Installations, Imports and Downloads\n",
        "\n",
        "The following code downloads all python modules needed for my Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: jax in /home/peter/.local/lib/python3.10/site-packages (0.4.31)\n",
            "Requirement already satisfied: numpy in /home/peter/.local/lib/python3.10/site-packages (1.24.1)\n",
            "Requirement already satisfied: pandas in /home/peter/.local/lib/python3.10/site-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /home/peter/.local/lib/python3.10/site-packages (3.9.2)\n",
            "Requirement already satisfied: scikit-learn in /home/peter/.local/lib/python3.10/site-packages (1.5.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.31,>=0.4.30 in /home/peter/.local/lib/python3.10/site-packages (from jax) (0.4.31)\n",
            "Requirement already satisfied: opt-einsum in /home/peter/.local/lib/python3.10/site-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/peter/.local/lib/python3.10/site-packages (from jax) (0.3.2)\n",
            "Requirement already satisfied: scipy>=1.10 in /home/peter/.local/lib/python3.10/site-packages (from jax) (1.13.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/peter/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/peter/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/peter/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/peter/.local/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/peter/.local/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/peter/.local/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/peter/.local/lib/python3.10/site-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/peter/.local/lib/python3.10/site-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: pillow>=8 in /home/peter/.local/lib/python3.10/site-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/peter/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/peter/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/peter/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/peter/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install jax numpy pandas matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once downloaded, these modules must be imported to be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1LecSdyiQyyh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "from jax import grad\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "FS = (9, 3)  # figure size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The AfriSenti dataset is used for this tutorial. The following block checks for the dataset in the current directory. If absent, the dataset is downloaded from the Github repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eM1X7TmNQyyi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directiory:  /home/peter/Documents/Hons/NLP/Ass1/afrisent-semeval-2023\n",
            "/home/peter/Documents/Hons/NLP/Ass1/afrisent-semeval-2023\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From https://github.com/afrisenti-semeval/afrisent-semeval-2023\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "# Download dataset\n",
        "PROJECT_DIR = os.getcwd() + '/afrisent-semeval-2023'\n",
        "print('Current directiory: ', PROJECT_DIR)\n",
        "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
        "\n",
        "if not os.path.isdir(PROJECT_DIR):\n",
        "  !git clone {PROJECT_GITHUB_URL}\n",
        "else:\n",
        "  %cd {PROJECT_DIR}\n",
        "  !git pull {PROJECT_GITHUB_URL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2YqUXBwQyyj"
      },
      "source": [
        "# 1. Data Loading & Preprocessing\n",
        "\n",
        "The AfriSenti dataset consists of tweets which have been human-labelled according to their emotional tone as either positive, negative, or neutral. It covers 14 languages spoken across the African continent. Each with unique annotations for training (*train.tsv*), validation (*dev.tsv*), and testing (*test.tsv*).\n",
        "\n",
        "### AfriSenti languages\n",
        "\n",
        "| No. | Language                     | Code | Country        |\n",
        "|-----|------------------------------|--------------|----------------|\n",
        "| 1   | Algerian Arabic              | arq          | Algeria        |\n",
        "| 2   | Amharic                      | amh          | Ethiopia       |\n",
        "| 3   | Hausa                        | hau          | Nigeria        |\n",
        "| 4   | Igbo                         | ibo          | Nigeria        |\n",
        "| 5   | Kinyarwanda                  | kin          | Rwanda         |\n",
        "| 6   | Moroccan Arabic/Darija       | ary          | Morocco        |\n",
        "| 7   | Mozambique Portuguese        | por        | Mozambique     |\n",
        "| 8   | Nigerian Pidgin              | pcm          | Nigeria        |\n",
        "| 9   | Oromo                        | orm          | Ethiopia       |\n",
        "| 10  | Swahili                      | swa          | Kenya/Tanzania |\n",
        "| 11  | Tigrinya                     | tir          | Ethiopia       |\n",
        "| 12  | Twi                          | twi          | Ghana          |\n",
        "| 13  | Xitsonga                     | tso          | Mozambique     |\n",
        "| 14  | Yoruba                       | yor          | Nigeria        |\n",
        "\n",
        "For my experiments, I decided to use the Hausa dataset. This is because it has both the largest and most balanced tweet samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pV7-4sf9Qyyj"
      },
      "outputs": [],
      "source": [
        "# Choose language\n",
        "language =  'hau'  # Can be ['arq', 'amh', 'hau', 'ibo', 'kin', 'ary', 'por', 'pcm', orm', 'swa', 'tir', 'twi', 'tso', 'yor']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbOLeaF9Qyyk"
      },
      "source": [
        "With a language specified, the different tweet samples can now be loaded to dataframes using pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yAADlbPPQyyk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directory:  /home/peter/Documents/Hons/NLP/Ass1/afrisent-semeval-2023/data/hau\n",
            "Train shape:  (14172, 2)\n",
            "Dev shape:  (2677, 2)\n",
            "Test shape:  (5303, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10865</th>\n",
              "      <td>@user Kaduna Allah yashiga lamarinku😥</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9901</th>\n",
              "      <td>@user Ga baki ga hanci😂😂 wai!!! Kyawu iya kyaw...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3999</th>\n",
              "      <td>@user Aradu gaskiya ne, tsohon ba abinda yake ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13372</th>\n",
              "      <td>@user Ga tsaro, ga taimako. Allah ya biya yara...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2655</th>\n",
              "      <td>@user Bbbc hausa da wulakanci kuke meye kuma w...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text     label\n",
              "10865              @user Kaduna Allah yashiga lamarinku😥  positive\n",
              "9901   @user Ga baki ga hanci😂😂 wai!!! Kyawu iya kyaw...  positive\n",
              "3999   @user Aradu gaskiya ne, tsohon ba abinda yake ...  negative\n",
              "13372  @user Ga tsaro, ga taimako. Allah ya biya yara...  positive\n",
              "2655   @user Bbbc hausa da wulakanci kuke meye kuma w...  negative"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load data\n",
        "DATA_DIR = f'{PROJECT_DIR}/data/{language}'\n",
        "print('Data directory: ', DATA_DIR)\n",
        "\n",
        "train_df = pd.read_csv(f'{DATA_DIR}/train.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
        "dev_df = pd.read_csv(f'{DATA_DIR}/dev.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
        "test_df = pd.read_csv(f'{DATA_DIR}/test.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
        "\n",
        "print('Train shape: ', train_df.shape)\n",
        "print('Dev shape: ', dev_df.shape)\n",
        "print('Test shape: ', test_df.shape)\n",
        "\n",
        "# Display data\n",
        "train_df.sample(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs3eTveIQyyl"
      },
      "source": [
        "<a name=\"section1_2\"></a>\n",
        "## 1.1. Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7PrIXhIQyyl"
      },
      "source": [
        "The aim is **binary classification** of the tweet's content as **positive** or **negative**. All tweets that are labelled as **neutral** must be filtered out before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zAWRyXM3Qyyl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape:  (9260, 2)\n",
            "Dev shape:  (1781, 2)\n",
            "Test shape:  (3514, 2)\n"
          ]
        }
      ],
      "source": [
        "# Discard neutral examples\n",
        "train_df = train_df[train_df['label'] != 'neutral']\n",
        "dev_df = dev_df[dev_df['label'] != 'neutral']\n",
        "test_df = test_df[test_df['label'] != 'neutral']\n",
        "\n",
        "print('Train shape: ', train_df.shape)\n",
        "print('Dev shape: ', dev_df.shape)\n",
        "print('Test shape: ', test_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pwpqmF3Qyyl"
      },
      "source": [
        "The following tasks are performed to further clean this dataset.\n",
        "\n",
        "* Discard neutral examples to perform binary classification.\n",
        "* Replace all urls with a special '[URL]' token.\n",
        "* Replace all numbers with a special '[NUM]' token.\n",
        "* Remove white extra whitespaces either side of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zcmqDXmDQyyl"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "    # Replace URLS with [URL]\n",
        "    text = re.sub(r'http\\S+', '[URL]', text)\n",
        "\n",
        "    # Replace numbers with [NUM]\n",
        "    text = re.sub(r'\\d+', '[NUM]', text)\n",
        "\n",
        "    # Remove trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(clean)\n",
        "dev_df['text'] = dev_df['text'].apply(clean)\n",
        "test_df['text'] = test_df['text'].apply(clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8yajd7_Qyyl"
      },
      "source": [
        "<a name=\"section1_3\"></a>\n",
        "## 1.2. Vocabulary construction\n",
        "\n",
        "A vocabulary is a set of unique words or tokens present in the text corpus. We refer to vocabulary items as **types** and to particular occurrences of these types in the dataset as **tokens**. I decided to use two tokenization methods to explore their potential impact on classifier performance\n",
        "\n",
        "For NLP purposes, we want to map each type in our vocabulary to an **index**, a unique number identifying that type. Later we can use this index to, for example, look up vector representations for our words using a lookup table. To achieve this, our vocabulary will be represented with three variables:\n",
        "* index2type: list of unique types in the vocabulary e.g. ['word1', 'word2', 'split3', ...]\n",
        "* type2index: dictionary mapping types to their index in the index2type vocabulary e.g. {'split1': 0, 'split2': 1, 'word3': 2, ...}\n",
        "* type2count: dictionary mapping types to the number of corresponding token occurences of that type in the training data e.g. {'word1': 1012, 'word2': 510, 'word3': 45, ...}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCksk5pjnfME"
      },
      "source": [
        "Next I perform whitespace tokenization. This is the strategy of splitting text on white spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "73Qyz6CQnTgS"
      },
      "outputs": [],
      "source": [
        "def whitespace_tokenize(sentences):\n",
        "    return [sentence.split() for sentence in sentences]\n",
        "\n",
        "# Store training data text as list of tweets\n",
        "train_corpus = train_df['text'].tolist()\n",
        "# Word tokenized corpus\n",
        "tokenized_train_corpus = whitespace_tokenize(train_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ_9skEDQyyl"
      },
      "source": [
        "* The function `` count_tokens `` takes a list of sentences as input and count the corpus size.\n",
        "* The function `` create_vocabulary `` takes a list of sentences as input and iteratively build a vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "42yNURWTQyym"
      },
      "outputs": [],
      "source": [
        "# Count number of tokens in corpus\n",
        "def count_tokens(sentences):\n",
        "    \"\"\"\n",
        "    Count number of tokens in corpus\n",
        "\n",
        "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
        "    return:\n",
        "        count: number of tokens in corpus\n",
        "    \"\"\"\n",
        "    count = sum(len(sent) for sent in sentences)\n",
        "    return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0jivEJjwQyym"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in corpus:  141969\n"
          ]
        }
      ],
      "source": [
        "num_tokens = count_tokens(tokenized_train_corpus)\n",
        "print('Number of tokens in corpus: ', num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I used the function 'create_type_counts' count the unique words (types) in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d4qPHK_pQyym"
      },
      "outputs": [],
      "source": [
        "# Collect type counts in corpus\n",
        "def create_type_counts(sentences):\n",
        "    \"\"\"\n",
        "    Count number of types in corpus\n",
        "\n",
        "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
        "    return:\n",
        "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
        "    \"\"\"\n",
        "    type2count = defaultdict(int)\n",
        "    for sentence in sentences:\n",
        "        for token in sentence:\n",
        "            type2count[token] += 1\n",
        "    return type2count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4aTRsvy-Qyym"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of types in corpus:  29569\n",
            "@user: 11313\n",
            "Allah: 3985\n",
            "da: 3914\n",
            "ya: 2921\n",
            "a: 1700\n",
            "ba: 1247\n"
          ]
        }
      ],
      "source": [
        "# Count occurances of a type in the corpus\n",
        "type2count = create_type_counts(tokenized_train_corpus)\n",
        "print('Number of types in corpus: ', len(type2count))\n",
        "\n",
        "# Sort types by counts\n",
        "type2count = dict(sorted(type2count.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "# Print first few types and counts\n",
        "for i, (type_, count) in enumerate(type2count.items()):\n",
        "    print(f'{type_}: {count}')\n",
        "    if i == 5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iwIP_prkQyyn"
      },
      "outputs": [],
      "source": [
        "# Create vocabulary\n",
        "def create_vocabulary(type2count, min_count):\n",
        "    \"\"\"\n",
        "    This function creates an indexed vocabulary from vocabulary counts and returns it as a list and a dictionary.\n",
        "\n",
        "    param:\n",
        "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
        "        min_count: minimum count of a word to be included in the vocabulary\n",
        "    return:\n",
        "        index2type: list of words in the vocabulary e.g. ['word1', 'word2', 'word3', ...]\n",
        "        type2index: dictionary mapping words to their index in the index2type vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
        "    \"\"\"\n",
        "    index2type = []\n",
        "    type2index = {}\n",
        "    idx = 0\n",
        "    for type,count in type2count.items():\n",
        "        if count >= min_count:\n",
        "            type2index[type] = idx\n",
        "            index2type.append(type)\n",
        "            idx += 1\n",
        "\n",
        "    # Add unknown token and padding\n",
        "    type2index['<UNK>'] = len(index2type)\n",
        "    index2type.append('<UNK>')\n",
        "    type2index['<PAD>'] = len(index2type)\n",
        "    index2type.append('<PAD>')\n",
        "\n",
        "    return index2type, type2index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qvuS2gCiQyyn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size:  29571\n",
            "First 10 words in the vocabulary:  ['@user', 'Allah', 'da', 'ya', 'a', 'ba', 'wannan', 'mu', 'ta', 'na']\n"
          ]
        }
      ],
      "source": [
        "index2type, type2index = create_vocabulary(type2count, min_count=1)\n",
        "\n",
        "print('Vocabulary size: ', len(index2type))\n",
        "print('First 10 words in the vocabulary: ', index2type[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGrwhRUkQyyn"
      },
      "source": [
        "<a name=\"section1_3\"></a>\n",
        "## 1.3. BPE tokenisation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmj6V7LBQyyn"
      },
      "source": [
        "Byte Pair Encoding (BPE) is a popular subword tokenisation algorithm in NLP. BPE and related algorithms have two parts:\n",
        "* A type learner that takes a raw training corpus and induces a vocabulary (a set of types) of prespecified size (e.g. 1000 subwords).\n",
        "* A token segmenter that takes a raw test sentence and tokenises it according to that subword vocabulary.\n",
        "\n",
        "The 'BPETokenizer' class combines these parts into one for ease of use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "r0VmGNvLQyyn"
      },
      "outputs": [],
      "source": [
        "# Implement BPE algorithm\n",
        "\n",
        "class BPETokenizer():\n",
        "\n",
        "    def __init__(self, sentences : list[str], vocab_size : int):\n",
        "        \"\"\"\n",
        "        Initialize the BPE tokenizer.\n",
        "\n",
        "        Args:\n",
        "            sentences (list[str]): list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
        "            vocab_size (int): The desired vocabulary size after training.\n",
        "        \"\"\"\n",
        "        self.sentences = sentences\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word_freqs = defaultdict(int)\n",
        "        self.pair_freqs = defaultdict(int)\n",
        "        self.splits = {}\n",
        "        self.merges = {}\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train the BPE tokenizer by iteratively merging the most frequent pairs of symbols.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary of merges in the format {(a, b): 'ab'}, where 'a' and 'b' are symbols merged into 'ab'.\n",
        "        \"\"\"\n",
        "        # Compute word frequencies and word splits\n",
        "        self.word_freqs = create_type_counts(self.sentences)\n",
        "\n",
        "        # Map each word to a list of its characters\n",
        "        for word in self.word_freqs:\n",
        "            self.splits[word] = list(word)\n",
        "\n",
        "        while (len(self.merges) < self.vocab_size):\n",
        "            self.pair_freqs = self.compute_pair_freqs()\n",
        "            # Stop if no pair has been found\n",
        "            if not self.pair_freqs:\n",
        "                print(\"No pairs to merge.\")\n",
        "                break\n",
        "            # Find most frequent pair\n",
        "            max_key = max(self.pair_freqs, key=self.pair_freqs.get)\n",
        "            self.merges[max_key] = \"\".join(max_key)\n",
        "            # Merge pair in dictionary of splits\n",
        "            print(f\"Merging: {max_key}, Current merges: {len(self.merges)}\")\n",
        "            self.splits = self.merge_pair(max_key[0], max_key[1])\n",
        "        return self.merges\n",
        "\n",
        "\n",
        "    def compute_pair_freqs(self):\n",
        "        \"\"\"\n",
        "        Compute the frequency of each pair of symbols in the corpus.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary of pairs and their frequencies in the format {(a, b): frequency}.\n",
        "        \"\"\"\n",
        "\n",
        "        pair_freq = defaultdict(int)\n",
        "        # Loop through each unique word in the corpus\n",
        "        for word, symbols in self.splits.items():\n",
        "            for i in range(len(symbols)-1):\n",
        "                # Increment the pair frequency by the word frequency\n",
        "                pair_freq[symbols[i],symbols[i+1]] += self.word_freqs[word]\n",
        "\n",
        "        return pair_freq\n",
        "\n",
        "\n",
        "    def merge_pair(self, a, b):\n",
        "        \"\"\"\n",
        "        Merge the given pair of symbols in all words where they appear adjacent.\n",
        "\n",
        "        Args:\n",
        "            a (str): The first symbol in the pair.\n",
        "            b (str): The second symbol in the pair.\n",
        "\n",
        "        Returns:\n",
        "            dict: The updated splits dictionary after merging.\n",
        "        \"\"\"\n",
        "        updated_splits = defaultdict(int)\n",
        "        \n",
        "        for word, split in self.splits.items():\n",
        "            encoded = []\n",
        "            i = 0\n",
        "            # Merge pair of symbols and replace word in sentence\n",
        "            while (i < len(split)):\n",
        "                if (i+1 != len(split)) and (split[i] == a) and (split[i+1] == b):\n",
        "                    # Add merged pair and skip next symbol\n",
        "                    encoded.append(a+b)\n",
        "                    i+=1\n",
        "                else:\n",
        "                    # Add current symbol\n",
        "                    encoded.append(split[i])\n",
        "                i += 1\n",
        "            updated_splits[word] = encoded            \n",
        "        return updated_splits\n",
        "\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize a given text using the trained BPE tokenizer.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text to be tokenized.\n",
        "\n",
        "        Returns:\n",
        "            list[str]: A list of tokens obtained after applying BPE tokenization.\n",
        "        \"\"\"\n",
        "\n",
        "        pre_tokenized_text = text.split()\n",
        "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
        "\n",
        "        for pair, merge in self.merges.items():\n",
        "            for idx, split in enumerate(splits_text):\n",
        "                i = 0\n",
        "                while i < len(split) - 1:\n",
        "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                        split = split[:i] + [merge] + split[i + 2 :]\n",
        "                    else:\n",
        "                        i += 1\n",
        "                splits_text[idx] = split\n",
        "        result = sum(splits_text, [])\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHk5wKzDQyyo"
      },
      "source": [
        "Running the following block trains a BPE tokeniser on our cleaned training dataframe. The merges learned are then applied to all dataframes. The final line shows how our vocabulary changes after subword tokenisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qUqUo_6HQyyo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging: ('a', 'n'), Current merges: 1\n",
            "Merging: ('e', 'r'), Current merges: 2\n",
            "Merging: ('u', 's'), Current merges: 3\n",
            "Merging: ('y', 'a'), Current merges: 4\n",
            "Merging: ('@', 'us'), Current merges: 5\n",
            "Merging: ('@us', 'er'), Current merges: 6\n",
            "Merging: ('d', 'a'), Current merges: 7\n",
            "Merging: ('k', 'a'), Current merges: 8\n",
            "Merging: ('i', 'n'), Current merges: 9\n",
            "Merging: ('m', 'a'), Current merges: 10\n",
            "Merging: ('l', 'a'), Current merges: 11\n",
            "Merging: ('b', 'a'), Current merges: 12\n",
            "Merging: ('w', 'a'), Current merges: 13\n",
            "Merging: ('s', 'a'), Current merges: 14\n",
            "Merging: ('t', 'a'), Current merges: 15\n",
            "Merging: ('l', 'la'), Current merges: 16\n",
            "Merging: ('lla', 'h'), Current merges: 17\n",
            "Merging: ('m', 'u'), Current merges: 18\n",
            "Merging: ('s', 'h'), Current merges: 19\n",
            "Merging: ('A', 'llah'), Current merges: 20\n",
            "Merging: ('n', 'a'), Current merges: 21\n",
            "Merging: ('r', 'a'), Current merges: 22\n",
            "Merging: ('s', 'u'), Current merges: 23\n",
            "Merging: ('k', 'u'), Current merges: 24\n",
            "Merging: ('h', 'a'), Current merges: 25\n",
            "Merging: ('g', 'a'), Current merges: 26\n",
            "Merging: ('k', 'i'), Current merges: 27\n",
            "Merging: ('\"', '\"'), Current merges: 28\n",
            "Merging: ('an', 'a'), Current merges: 29\n",
            "Merging: ('k', 'e'), Current merges: 30\n",
            "Merging: ('z', 'a'), Current merges: 31\n",
            "Merging: ('y', 'i'), Current merges: 32\n",
            "Merging: ('.', '.'), Current merges: 33\n",
            "Merging: ('n', 'an'), Current merges: 34\n",
            "Merging: ('w', 'an'), Current merges: 35\n",
            "Merging: ('i', 'ya'), Current merges: 36\n",
            "Merging: ('u', 'n'), Current merges: 37\n",
            "Merging: ('f', 'a'), Current merges: 38\n",
            "Merging: ('c', 'i'), Current merges: 39\n",
            "Merging: ('sh', 'i'), Current merges: 40\n",
            "Merging: ('k', 'o'), Current merges: 41\n",
            "Merging: ('c', 'e'), Current merges: 42\n",
            "Merging: ('r', 'e'), Current merges: 43\n",
            "Merging: ('r', 'i'), Current merges: 44\n",
            "Merging: ('j', 'i'), Current merges: 45\n",
            "Merging: ('b', 'an'), Current merges: 46\n",
            "Merging: ('a', 'i'), Current merges: 47\n",
            "Merging: ('n', 'e'), Current merges: 48\n",
            "Merging: ('k', 'an'), Current merges: 49\n",
            "Merging: ('k', 'in'), Current merges: 50\n",
            "Merging: ('😂', '😂'), Current merges: 51\n",
            "Merging: ('y', 'an'), Current merges: 52\n",
            "Merging: ('\"\"', '\"\"'), Current merges: 53\n",
            "Merging: ('e', 'n'), Current merges: 54\n",
            "Merging: ('o', 'n'), Current merges: 55\n",
            "Merging: ('d', 'an'), Current merges: 56\n",
            "Merging: ('d', 'u'), Current merges: 57\n",
            "Merging: ('a', 'b'), Current merges: 58\n",
            "Merging: ('f', 'i'), Current merges: 59\n",
            "Merging: ('da', 'i'), Current merges: 60\n",
            "Merging: ('ka', 'r'), Current merges: 61\n",
            "Merging: ('😭', '😭'), Current merges: 62\n",
            "Merging: ('a', 'm'), Current merges: 63\n",
            "Merging: ('t', 'u'), Current merges: 64\n",
            "Merging: ('a', 'r'), Current merges: 65\n",
            "Merging: ('a', 'l'), Current merges: 66\n",
            "Merging: ('r', 'in'), Current merges: 67\n",
            "Merging: ('b', 'u'), Current merges: 68\n",
            "Merging: ('wa', 'i'), Current merges: 69\n",
            "Merging: ('sa', 'i'), Current merges: 70\n",
            "Merging: ('s', 'o'), Current merges: 71\n",
            "Merging: ('wan', 'nan'), Current merges: 72\n",
            "Merging: ('l', 'i'), Current merges: 73\n",
            "Merging: ('m', 'an'), Current merges: 74\n",
            "Merging: ('u', 'wa'), Current merges: 75\n",
            "Merging: ('ku', 'ma'), Current merges: 76\n",
            "Merging: ('h', 'i'), Current merges: 77\n",
            "Merging: ('d', 'in'), Current merges: 78\n",
            "Merging: ('Y', 'a'), Current merges: 79\n",
            "Merging: ('m', 'ana'), Current merges: 80\n",
            "Merging: ('m', 'e'), Current merges: 81\n",
            "Merging: ('s', 'an'), Current merges: 82\n",
            "Merging: ('sh', 'e'), Current merges: 83\n",
            "Merging: ('t', 'o'), Current merges: 84\n",
            "Merging: ('ma', 'i'), Current merges: 85\n",
            "Merging: ('g', 'i'), Current merges: 86\n",
            "Merging: ('d', 'i'), Current merges: 87\n",
            "Merging: ('b', 'i'), Current merges: 88\n",
            "Merging: ('t', 'sa'), Current merges: 89\n",
            "Merging: ('t', 'an'), Current merges: 90\n",
            "Merging: ('M', 'a'), Current merges: 91\n",
            "Merging: ('w', 'o'), Current merges: 92\n",
            "Merging: ('sh', 'a'), Current merges: 93\n",
            "Merging: ('in', 'a'), Current merges: 94\n",
            "Merging: ('🙏', '🙏'), Current merges: 95\n",
            "Merging: ('ya', 'u'), Current merges: 96\n",
            "Merging: ('ki', 'ya'), Current merges: 97\n",
            "Merging: ('N', 'i'), Current merges: 98\n",
            "Merging: ('ka', 'i'), Current merges: 99\n",
            "Merging: ('g', 'an'), Current merges: 100\n",
            "Merging: ('m', 'in'), Current merges: 101\n",
            "Merging: ('l', 'l'), Current merges: 102\n",
            "Merging: ('m', 'i'), Current merges: 103\n",
            "Merging: ('er', 'i'), Current merges: 104\n",
            "Merging: ('K', 'a'), Current merges: 105\n",
            "Merging: ('ba', 'r'), Current merges: 106\n",
            "Merging: ('U', 'M'), Current merges: 107\n",
            "Merging: ('🤣', '🤣'), Current merges: 108\n",
            "Merging: ('B', 'a'), Current merges: 109\n",
            "Merging: ('\"\"\"\"', '\"\"\"\"'), Current merges: 110\n",
            "Merging: ('d', 'o'), Current merges: 111\n",
            "Merging: ('[', 'N'), Current merges: 112\n",
            "Merging: ('[N', 'UM'), Current merges: 113\n",
            "Merging: ('[NUM', ']'), Current merges: 114\n",
            "Merging: ('h', 'an'), Current merges: 115\n",
            "Merging: ('s', 'i'), Current merges: 116\n",
            "Merging: ('h', 'e'), Current merges: 117\n",
            "Merging: ('sa', 'r'), Current merges: 118\n",
            "Merging: ('j', 'a'), Current merges: 119\n",
            "Merging: ('ka', 'ra'), Current merges: 120\n",
            "Merging: ('D', 'a'), Current merges: 121\n",
            "Merging: ('y', 'e'), Current merges: 122\n",
            "Merging: ('..', '.'), Current merges: 123\n",
            "Merging: ('g', 'o'), Current merges: 124\n",
            "Merging: ('t', 'i'), Current merges: 125\n",
            "Merging: ('M', 'u'), Current merges: 126\n",
            "Merging: ('ya', 'r'), Current merges: 127\n",
            "Merging: ('sh', 'in'), Current merges: 128\n",
            "Merging: ('ya', 'sa'), Current merges: 129\n",
            "Merging: ('c', 'in'), Current merges: 130\n",
            "Merging: ('m', 'us'), Current merges: 131\n",
            "Merging: ('mu', 'n'), Current merges: 132\n",
            "Merging: ('l', 'h'), Current merges: 133\n",
            "Merging: ('s', 'kiya'), Current merges: 134\n",
            "Merging: ('g', 'eri'), Current merges: 135\n",
            "Merging: ('ya', 'yi'), Current merges: 136\n",
            "Merging: ('l', 'e'), Current merges: 137\n",
            "Merging: ('ta', 'r'), Current merges: 138\n",
            "Merging: ('K', 'u'), Current merges: 139\n",
            "Merging: (\"'\", 'a'), Current merges: 140\n",
            "Merging: ('geri', 'a'), Current merges: 141\n",
            "Merging: ('ma', 'r'), Current merges: 142\n",
            "Merging: ('d', 'e'), Current merges: 143\n",
            "Merging: ('k', 'yau'), Current merges: 144\n",
            "Merging: ('n', 'i'), Current merges: 145\n",
            "Merging: ('H', 'a'), Current merges: 146\n",
            "Merging: ('r', 'u'), Current merges: 147\n",
            "Merging: ('da', 'u'), Current merges: 148\n",
            "Merging: ('r', 'o'), Current merges: 149\n",
            "Merging: ('ku', 'n'), Current merges: 150\n",
            "Merging: ('a', 'llah'), Current merges: 151\n",
            "Merging: ('mu', 'na'), Current merges: 152\n",
            "Merging: ('W', 'an'), Current merges: 153\n",
            "Merging: ('z', 'u'), Current merges: 154\n",
            "Merging: ('g', 'u'), Current merges: 155\n",
            "Merging: ('ha', 'r'), Current merges: 156\n",
            "Merging: ('G', 'a'), Current merges: 157\n",
            "Merging: ('y', 'in'), Current merges: 158\n",
            "Merging: ('Ni', 'geria'), Current merges: 159\n",
            "Merging: ('su', 'n'), Current merges: 160\n",
            "Merging: ('m', 'ma'), Current merges: 161\n",
            "Merging: ('wan', 'i'), Current merges: 162\n",
            "Merging: ('za', 'i'), Current merges: 163\n",
            "Merging: ('du', 'k'), Current merges: 164\n",
            "Merging: ('ka', 'wai'), Current merges: 165\n",
            "Merging: ('r', 'an'), Current merges: 166\n",
            "Merging: ('ka', 'wo'), Current merges: 167\n",
            "Merging: ('mus', 'u'), Current merges: 168\n",
            "Merging: ('D', 'an'), Current merges: 169\n",
            "Merging: ('y', 'o'), Current merges: 170\n",
            "Merging: ('b', 'e'), Current merges: 171\n",
            "Merging: ('ma', 'ta'), Current merges: 172\n",
            "Merging: ('h', 'u'), Current merges: 173\n",
            "Merging: ('ga', 'ba'), Current merges: 174\n",
            "Merging: ('T', 'o'), Current merges: 175\n",
            "Merging: ('t', 's'), Current merges: 176\n",
            "Merging: ('k', 'wa'), Current merges: 177\n",
            "Merging: ('ab', 'in'), Current merges: 178\n",
            "Merging: ('t', 'un'), Current merges: 179\n",
            "Merging: ('R', 'a'), Current merges: 180\n",
            "Merging: ('ka', 'sa'), Current merges: 181\n",
            "Merging: ('b', 'o'), Current merges: 182\n",
            "Merging: ('da', 'ya'), Current merges: 183\n",
            "Merging: ('w', 'lh'), Current merges: 184\n",
            "Merging: ('l', 'o'), Current merges: 185\n",
            "Merging: ('I', 'n'), Current merges: 186\n",
            "Merging: ('mu', 'tu'), Current merges: 187\n",
            "Merging: ('i', 's'), Current merges: 188\n",
            "Merging: ('😂😂', '😂'), Current merges: 189\n",
            "Merging: ('ci', 'kin'), Current merges: 190\n",
            "Merging: ('A', 'l'), Current merges: 191\n",
            "Merging: ('a', 'ka'), Current merges: 192\n",
            "Merging: ('ha', 'ri'), Current merges: 193\n",
            "Merging: ('ke', 'nan'), Current merges: 194\n",
            "Merging: ('j', 'e'), Current merges: 195\n",
            "Merging: ('ma', 'su'), Current merges: 196\n",
            "Merging: ('d', 'un'), Current merges: 197\n",
            "Merging: ('..', '..'), Current merges: 198\n",
            "Merging: ('ya', 'ke'), Current merges: 199\n",
            "Merging: ('ha', 'ka'), Current merges: 200\n",
            "Merging: ('llah', 'i'), Current merges: 201\n",
            "Merging: ('g', 'e'), Current merges: 202\n",
            "Merging: ('us', 'a'), Current merges: 203\n",
            "Merging: ('w', 'u'), Current merges: 204\n",
            "Merging: ('m', 'm'), Current merges: 205\n",
            "Merging: ('fa', 'r'), Current merges: 206\n",
            "Merging: ('U', 'R'), Current merges: 207\n",
            "Merging: ('sa', 'u'), Current merges: 208\n",
            "Merging: ('f', 'iya'), Current merges: 209\n",
            "Merging: ('ha', 'ma'), Current merges: 210\n",
            "Merging: ('N', 'a'), Current merges: 211\n",
            "Merging: ('y', 'ana'), Current merges: 212\n",
            "Merging: ('wa', 'r'), Current merges: 213\n",
            "Merging: ('h', 'h'), Current merges: 214\n",
            "Merging: ('q', 'a'), Current merges: 215\n",
            "Merging: ('s', 'e'), Current merges: 216\n",
            "Merging: ('!', '!'), Current merges: 217\n",
            "Merging: ('am', 'ma'), Current merges: 218\n",
            "Merging: ('[', 'UR'), Current merges: 219\n",
            "Merging: ('[UR', 'L'), Current merges: 220\n",
            "Merging: ('[URL', ']'), Current merges: 221\n",
            "Merging: ('h', 'o'), Current merges: 222\n",
            "Merging: ('L', 'A'), Current merges: 223\n",
            "Merging: ('j', 'in'), Current merges: 224\n",
            "Merging: ('mu', 'tan'), Current merges: 225\n",
            "Merging: ('f', 'e'), Current merges: 226\n",
            "Merging: ('Wan', 'nan'), Current merges: 227\n",
            "Merging: ('ta', 'la'), Current merges: 228\n",
            "Merging: ('ga', 'skiya'), Current merges: 229\n",
            "Merging: ('l', 'in'), Current merges: 230\n",
            "Merging: ('a', 'ke'), Current merges: 231\n",
            "Merging: ('o', 'r'), Current merges: 232\n",
            "Merging: ('W', 'a'), Current merges: 233\n",
            "Merging: ('u', 'ban'), Current merges: 234\n",
            "Merging: ('i', 'dan'), Current merges: 235\n",
            "Merging: ('i', 'rin'), Current merges: 236\n",
            "Merging: ('su', 'ka'), Current merges: 237\n",
            "Merging: ('🇳', '🇬'), Current merges: 238\n",
            "Merging: ('d', 'da'), Current merges: 239\n",
            "Merging: ('f', 'in'), Current merges: 240\n",
            "Merging: ('u', 'na'), Current merges: 241\n",
            "Merging: ('ka', 'she'), Current merges: 242\n",
            "Merging: ('z', 'an'), Current merges: 243\n",
            "Merging: ('ban', 'za'), Current merges: 244\n",
            "Merging: ('da', 'ga'), Current merges: 245\n",
            "Merging: ('uwa', 'r'), Current merges: 246\n",
            "Merging: ('in', 'g'), Current merges: 247\n",
            "Merging: ('A', 'L'), Current merges: 248\n",
            "Merging: ('🤲', '🤲'), Current merges: 249\n",
            "Merging: ('A', 'n'), Current merges: 250\n",
            "Merging: ('fa', 'ra'), Current merges: 251\n",
            "Merging: ('K', 'ai'), Current merges: 252\n",
            "Merging: ('ka', 're'), Current merges: 253\n",
            "Merging: ('ma', 'sa'), Current merges: 254\n",
            "Merging: ('g', 'ana'), Current merges: 255\n",
            "Merging: ('A', 'i'), Current merges: 256\n",
            "Merging: ('s', 'on'), Current merges: 257\n",
            "Merging: ('ga', 'ban'), Current merges: 258\n",
            "Merging: ('😭😭', '😭'), Current merges: 259\n",
            "Merging: ('Ku', 'ma'), Current merges: 260\n",
            "Merging: ('ab', 'un'), Current merges: 261\n",
            "Merging: ('ba', 'ki'), Current merges: 262\n",
            "Merging: ('t', 'h'), Current merges: 263\n",
            "Merging: ('S', 'u'), Current merges: 264\n",
            "Merging: ('A', 'me'), Current merges: 265\n",
            "Merging: ('LA', 'H'), Current merges: 266\n",
            "Merging: ('ba', 'bu'), Current merges: 267\n",
            "Merging: ('k', 'wai'), Current merges: 268\n",
            "Merging: ('wan', 'da'), Current merges: 269\n",
            "Merging: ('iya', 'r'), Current merges: 270\n",
            "Merging: ('da', 'ma'), Current merges: 271\n",
            "Merging: ('ba', 'su'), Current merges: 272\n",
            "Merging: ('Ga', 'skiya'), Current merges: 273\n",
            "Merging: ('s', 't'), Current merges: 274\n",
            "Merging: ('ba', 'ya'), Current merges: 275\n",
            "Merging: ('z', 'o'), Current merges: 276\n",
            "Merging: ('b', 'in'), Current merges: 277\n",
            "Merging: ('a', 'u'), Current merges: 278\n",
            "Merging: ('ba', 'mu'), Current merges: 279\n",
            "Merging: ('🤔', '🤔'), Current merges: 280\n",
            "Merging: ('su', 'ke'), Current merges: 281\n",
            "Merging: ('la', 'fiya'), Current merges: 282\n",
            "Merging: ('shin', 'e'), Current merges: 283\n",
            "Merging: ('\\u200d', '♂'), Current merges: 284\n",
            "Merging: ('sh', 'en'), Current merges: 285\n",
            "Merging: ('AL', 'LAH'), Current merges: 286\n",
            "Merging: ('ta', 'i'), Current merges: 287\n",
            "Merging: ('sh', 'u'), Current merges: 288\n",
            "Merging: ('B', 'u'), Current merges: 289\n",
            "Merging: ('ma', 'ka'), Current merges: 290\n",
            "Merging: ('K', 'o'), Current merges: 291\n",
            "Merging: ('ba', 'i'), Current merges: 292\n",
            "Merging: ('\\u200d♂', '️'), Current merges: 293\n",
            "Merging: ('re', 'wa'), Current merges: 294\n",
            "Merging: ('j', 'an'), Current merges: 295\n",
            "Merging: ('ha', 'i'), Current merges: 296\n",
            "Merging: ('gan', 'in'), Current merges: 297\n",
            "Merging: ('ab', 'u'), Current merges: 298\n",
            "Merging: ('t', 'so'), Current merges: 299\n",
            "Merging: ('s', 's'), Current merges: 300\n",
            "Merging: ('T', 'a'), Current merges: 301\n",
            "Merging: ('I', 'na'), Current merges: 302\n",
            "Merging: ('\"\"\"\"\"\"\"\"', '\"\"\"\"\"\"\"\"'), Current merges: 303\n",
            "Merging: ('ko', 'n'), Current merges: 304\n",
            "Merging: ('ma', 'gana'), Current merges: 305\n",
            "Merging: ('y', 'u'), Current merges: 306\n",
            "Merging: ('ka', 'sar'), Current merges: 307\n",
            "Merging: ('yo', 'u'), Current merges: 308\n",
            "Merging: ('Ame', 'en'), Current merges: 309\n",
            "Merging: ('m', 'o'), Current merges: 310\n",
            "Merging: ('dun', 'iya'), Current merges: 311\n",
            "Merging: ('y', 'ya'), Current merges: 312\n",
            "Merging: ('d', 'on'), Current merges: 313\n",
            "Merging: ('han', 'ka'), Current merges: 314\n",
            "Merging: ('musu', 'l'), Current merges: 315\n",
            "Merging: ('la', 'i'), Current merges: 316\n",
            "Merging: ('A', 'mma'), Current merges: 317\n",
            "Merging: ('su', 'na'), Current merges: 318\n",
            "Merging: ('mutan', 'e'), Current merges: 319\n",
            "Merging: ('🙄', '🙄'), Current merges: 320\n",
            "Merging: ('tala', 'ka'), Current merges: 321\n",
            "Merging: ('bar', 'ka'), Current merges: 322\n",
            "Merging: ('a', 'd'), Current merges: 323\n",
            "Merging: ('S', 'ai'), Current merges: 324\n",
            "Merging: ('ko', 'wa'), Current merges: 325\n",
            "Merging: ('😡', '😡'), Current merges: 326\n",
            "Merging: ('h', 'ana'), Current merges: 327\n",
            "Merging: ('kar', 'ya'), Current merges: 328\n",
            "Merging: ('f', 'an'), Current merges: 329\n",
            "Merging: ('v', 'e'), Current merges: 330\n",
            "Merging: ('wa', 'ta'), Current merges: 331\n",
            "Merging: ('gi', 'ji'), Current merges: 332\n",
            "Merging: ('ji', 'kan'), Current merges: 333\n",
            "Merging: ('B', 'B'), Current merges: 334\n",
            "Merging: ('gan', 'i'), Current merges: 335\n",
            "Merging: ('ma', 'tsa'), Current merges: 336\n",
            "Merging: ('sa', 'ka'), Current merges: 337\n",
            "Merging: ('💯', '💯'), Current merges: 338\n",
            "Merging: ('t', 'e'), Current merges: 339\n",
            "Merging: ('m', 'na'), Current merges: 340\n",
            "Merging: ('g', 'wa'), Current merges: 341\n",
            "Merging: ('A', 'N'), Current merges: 342\n",
            "Merging: ('w', 'i'), Current merges: 343\n",
            "Merging: ('i', 'ta'), Current merges: 344\n",
            "Merging: ('ba', ','), Current merges: 345\n",
            "Merging: ('L', 'a'), Current merges: 346\n",
            "Merging: ('ce', 'wa'), Current merges: 347\n",
            "Merging: ('Y', 'an'), Current merges: 348\n",
            "Merging: ('x', 'a'), Current merges: 349\n",
            "Merging: ('f', 'u'), Current merges: 350\n",
            "Merging: ('M', 'ai'), Current merges: 351\n",
            "Merging: ('kyau', 'ta'), Current merges: 352\n",
            "Merging: ('ban', 'e'), Current merges: 353\n",
            "Merging: ('ra', 'hama'), Current merges: 354\n",
            "Merging: ('c', 'o'), Current merges: 355\n",
            "Merging: ('shi', 'ga'), Current merges: 356\n",
            "Merging: ('a', 'kan'), Current merges: 357\n",
            "Merging: ('l', 'u'), Current merges: 358\n",
            "Merging: ('BB', 'C'), Current merges: 359\n",
            "Merging: ('😠', '😠'), Current merges: 360\n",
            "Merging: ('ka', 'yi'), Current merges: 361\n",
            "Merging: ('😏', '😏'), Current merges: 362\n",
            "Merging: ('shi', 'r'), Current merges: 363\n",
            "Merging: ('yan', 'zu'), Current merges: 364\n",
            "Merging: ('n', 'u'), Current merges: 365\n",
            "Merging: ('mu', 'l'), Current merges: 366\n",
            "Merging: ('la', 'm'), Current merges: 367\n",
            "Merging: ('n', 'o'), Current merges: 368\n",
            "Merging: ('A', 'R'), Current merges: 369\n",
            "Merging: ('sa', 'mu'), Current merges: 370\n",
            "Merging: ('ba', 'da'), Current merges: 371\n",
            "Merging: ('hanka', 'li'), Current merges: 372\n",
            "Merging: ('dau', 'ka'), Current merges: 373\n",
            "Merging: ('e', 'en'), Current merges: 374\n",
            "Merging: ('ta', 'ba'), Current merges: 375\n",
            "Merging: ('j', 'er'), Current merges: 376\n",
            "Merging: ('t', 'y'), Current merges: 377\n",
            "Merging: ('ba', 'yan'), Current merges: 378\n",
            "Merging: ('Z', 'a'), Current merges: 379\n",
            "Merging: ('c', 'hi'), Current merges: 380\n",
            "Merging: ('ka', 'mata'), Current merges: 381\n",
            "Merging: ('ku', 'wa'), Current merges: 382\n",
            "Merging: ('Bu', 'hari'), Current merges: 383\n",
            "Merging: ('ci', 'ki'), Current merges: 384\n",
            "Merging: ('ba', 'ta'), Current merges: 385\n",
            "Merging: ('sa', 'bo'), Current merges: 386\n",
            "Merging: ('🙏🙏', '🙏'), Current merges: 387\n",
            "Merging: ('😭😭', '😭😭'), Current merges: 388\n",
            "Merging: ('za', 'mu'), Current merges: 389\n",
            "Merging: ('t', 'he'), Current merges: 390\n",
            "Merging: ('a', 'kwai'), Current merges: 391\n",
            "Merging: ('w', 'w'), Current merges: 392\n",
            "Merging: ('i', 'sa'), Current merges: 393\n",
            "Merging: ('To', 'h'), Current merges: 394\n",
            "Merging: ('za', 'man'), Current merges: 395\n",
            "Merging: ('Mu', 'na'), Current merges: 396\n",
            "Merging: ('sar', 'ki'), Current merges: 397\n",
            "Merging: ('S', 'a'), Current merges: 398\n",
            "Merging: ('A', 'min'), Current merges: 399\n",
            "Merging: ('c', 'an'), Current merges: 400\n",
            "Merging: ('wa', 'llahi'), Current merges: 401\n",
            "Merging: ('c', 'u'), Current merges: 402\n",
            "Merging: ('do', 'min'), Current merges: 403\n",
            "Merging: ('fa', 'da'), Current merges: 404\n",
            "Merging: ('ha', 'kan'), Current merges: 405\n",
            "Merging: ('ba', '.'), Current merges: 406\n",
            "Merging: ('Ma', 'sha'), Current merges: 407\n",
            "Merging: ('t', 'in'), Current merges: 408\n",
            "Merging: ('ka', 'm'), Current merges: 409\n",
            "Merging: ('za', 'su'), Current merges: 410\n",
            "Merging: ('za', 'ma'), Current merges: 411\n",
            "Merging: ('sabo', 'da'), Current merges: 412\n",
            "Merging: ('h', 'in'), Current merges: 413\n",
            "Merging: ('ya', 'wa'), Current merges: 414\n",
            "Merging: ('D', 'u'), Current merges: 415\n",
            "Merging: ('S', 'hi'), Current merges: 416\n",
            "Merging: ('o', 'd'), Current merges: 417\n",
            "Merging: ('sa', 'ra'), Current merges: 418\n",
            "Merging: ('na', 'bi'), Current merges: 419\n",
            "Merging: ('😂😂', '😂😂'), Current merges: 420\n",
            "Merging: ('ha', 'u'), Current merges: 421\n",
            "Merging: ('da', 'mu'), Current merges: 422\n",
            "Merging: ('abin', 'da'), Current merges: 423\n",
            "Merging: ('a', 'yi'), Current merges: 424\n",
            "Merging: ('In', 'na'), Current merges: 425\n",
            "Merging: ('W', 'ai'), Current merges: 426\n",
            "Merging: ('ka', 'mar'), Current merges: 427\n",
            "Merging: ('ra', 'i'), Current merges: 428\n",
            "Merging: ('l', 'm'), Current merges: 429\n",
            "Merging: ('k', 'ana'), Current merges: 430\n",
            "Merging: ('?', '?'), Current merges: 431\n",
            "Merging: ('😁', '😁'), Current merges: 432\n",
            "Merging: ('w', 'e'), Current merges: 433\n",
            "Merging: ('kar', 'shen'), Current merges: 434\n",
            "Merging: ('U', 'ban'), Current merges: 435\n",
            "Merging: ('Ra', 'hama'), Current merges: 436\n",
            "Merging: ('ba', 'ka'), Current merges: 437\n",
            "Merging: ('ba', 'rin'), Current merges: 438\n",
            "Merging: ('ra', 'shin'), Current merges: 439\n",
            "Merging: ('da', 'di'), Current merges: 440\n",
            "Merging: ('shu', 'gaban'), Current merges: 441\n",
            "Merging: ('li', 'llahi'), Current merges: 442\n",
            "Merging: ('k', 'hai'), Current merges: 443\n",
            "Merging: ('bu', 'hari'), Current merges: 444\n",
            "Merging: ('❤', '️'), Current merges: 445\n",
            "Merging: ('mutu', 'm'), Current merges: 446\n",
            "Merging: ('b', 'ba'), Current merges: 447\n",
            "Merging: ('i', 'r'), Current merges: 448\n",
            "Merging: ('ko', 'ma'), Current merges: 449\n",
            "Merging: ('ra', 'r'), Current merges: 450\n",
            "Merging: ('ta', 'yi'), Current merges: 451\n",
            "Merging: ('I', 'N'), Current merges: 452\n",
            "Merging: ('h', 'ma'), Current merges: 453\n",
            "Merging: ('ku', 'din'), Current merges: 454\n",
            "Merging: ('ba', 'shi'), Current merges: 455\n",
            "Merging: ('gi', 'da'), Current merges: 456\n",
            "Merging: ('y', 'en'), Current merges: 457\n",
            "Merging: ('K', 'ar'), Current merges: 458\n",
            "Merging: ('🤣🤣', '🤣'), Current merges: 459\n",
            "Merging: ('s', 'ki'), Current merges: 460\n",
            "Merging: ('mu', 'ke'), Current merges: 461\n",
            "Merging: ('ana', 'r'), Current merges: 462\n",
            "Merging: ('jer', 'iya'), Current merges: 463\n",
            "Merging: ('b', 'iya'), Current merges: 464\n",
            "Merging: ('👏', '👏'), Current merges: 465\n",
            "Merging: ('ya', 'kara'), Current merges: 466\n",
            "Merging: ('ko', 'mai'), Current merges: 467\n",
            "Merging: ('K', 'an'), Current merges: 468\n",
            "Merging: ('F', 'a'), Current merges: 469\n",
            "Merging: ('ha', 'la'), Current merges: 470\n",
            "Merging: ('in', 'e'), Current merges: 471\n",
            "Merging: ('ya', 'ra'), Current merges: 472\n",
            "Merging: ('t', 'er'), Current merges: 473\n",
            "Merging: ('😂', '🤣'), Current merges: 474\n",
            "Merging: ('ll', 'h'), Current merges: 475\n",
            "Merging: ('lo', 'ka'), Current merges: 476\n",
            "Merging: ('k', 'y'), Current merges: 477\n",
            "Merging: ('o', 'f'), Current merges: 478\n",
            "Merging: ('ku', 'ke'), Current merges: 479\n",
            "Merging: ('l', 'an'), Current merges: 480\n",
            "Merging: ('z', 'uwa'), Current merges: 481\n",
            "Merging: ('ra', 'y'), Current merges: 482\n",
            "Merging: ('ta', 'ke'), Current merges: 483\n",
            "Merging: ('ya', 'fi'), Current merges: 484\n",
            "Merging: ('si', 'ra'), Current merges: 485\n",
            "Merging: ('khai', 'ri'), Current merges: 486\n",
            "Merging: ('mu', 'r'), Current merges: 487\n",
            "Merging: ('ai', 'kin'), Current merges: 488\n",
            "Merging: ('ku', 'na'), Current merges: 489\n",
            "Merging: ('a', 'da'), Current merges: 490\n",
            "Merging: ('Y', 'A'), Current merges: 491\n",
            "Merging: ('gi', 'r'), Current merges: 492\n",
            "Merging: ('da', 'shi'), Current merges: 493\n",
            "Merging: ('😢', '😢'), Current merges: 494\n",
            "Merging: ('r', 'uwa'), Current merges: 495\n",
            "Merging: ('l', 'y'), Current merges: 496\n",
            "Merging: ('p', 'p'), Current merges: 497\n",
            "Merging: ('ab', 'o'), Current merges: 498\n",
            "Merging: ('😎', '😎'), Current merges: 499\n",
            "Merging: ('matsa', 'la'), Current merges: 500\n",
            "Merging: ('u', 'm'), Current merges: 501\n",
            "Merging: ('fa', 'tan'), Current merges: 502\n",
            "Merging: ('p', 'i'), Current merges: 503\n",
            "Merging: ('ma', 'ki'), Current merges: 504\n",
            "Merging: ('u', 'r'), Current merges: 505\n",
            "Merging: ('j', 'iya'), Current merges: 506\n",
            "Merging: ('re', 'n'), Current merges: 507\n",
            "Merging: ('v', 'er'), Current merges: 508\n",
            "Merging: ('ba', 'wa'), Current merges: 509\n",
            "Merging: ('g', 'en'), Current merges: 510\n",
            "Merging: ('😅', '😅'), Current merges: 511\n",
            "Merging: ('ka', 'ce'), Current merges: 512\n",
            "Merging: ('ya', 'dda'), Current merges: 513\n",
            "Merging: ('ka', 'ma'), Current merges: 514\n",
            "Merging: ('a', 'ce'), Current merges: 515\n",
            "Merging: ('w', 'ana'), Current merges: 516\n",
            "Merging: ('ha', 'ha'), Current merges: 517\n",
            "Merging: ('ka', 'ta'), Current merges: 518\n",
            "Merging: ('f', 'or'), Current merges: 519\n",
            "Merging: ('H', 'mm'), Current merges: 520\n",
            "Merging: ('ga', 'ri'), Current merges: 521\n",
            "Merging: ('al', 'barka'), Current merges: 522\n",
            "Merging: ('q', 'ar'), Current merges: 523\n",
            "Merging: ('a', 'rewa'), Current merges: 524\n",
            "Merging: ('r', 'on'), Current merges: 525\n",
            "Merging: ('🙏', '🏻'), Current merges: 526\n",
            "Merging: ('ba', 'kin'), Current merges: 527\n",
            "Merging: ('ba', 'ba'), Current merges: 528\n",
            "Merging: ('gi', 'dan'), Current merges: 529\n",
            "Merging: ('A', 'na'), Current merges: 530\n",
            "Merging: ('ci', 'ka'), Current merges: 531\n",
            "Merging: ('qa', 'ra'), Current merges: 532\n",
            "Merging: ('x', 'u'), Current merges: 533\n",
            "Merging: ('c', 'a'), Current merges: 534\n",
            "Merging: ('ga', 'ma'), Current merges: 535\n",
            "Merging: ('s', 'ky'), Current merges: 536\n",
            "Merging: ('am', 'in'), Current merges: 537\n",
            "Merging: ('ha', 'm'), Current merges: 538\n",
            "Merging: ('da', 'ra'), Current merges: 539\n",
            "Merging: ('kin', 'a'), Current merges: 540\n",
            "Merging: ('wa', 'su'), Current merges: 541\n",
            "Merging: ('ru', 'wan'), Current merges: 542\n",
            "Merging: ('da', 'wo'), Current merges: 543\n",
            "Merging: ('da', 'r'), Current merges: 544\n",
            "Merging: ('a', 'a'), Current merges: 545\n",
            "Merging: ('in', 'na'), Current merges: 546\n",
            "Merging: ('tsa', 're'), Current merges: 547\n",
            "Merging: ('uban', 'giji'), Current merges: 548\n",
            "Merging: ('gwa', 'mna'), Current merges: 549\n",
            "Merging: ('😀', '😀'), Current merges: 550\n",
            "Merging: ('W', 'allah'), Current merges: 551\n",
            "Merging: ('s', 'in'), Current merges: 552\n",
            "Merging: ('da', 'ce'), Current merges: 553\n",
            "Merging: ('fi', 'lm'), Current merges: 554\n",
            "Merging: ('I', 'dan'), Current merges: 555\n",
            "Merging: ('dauka', 'ka'), Current merges: 556\n",
            "Merging: ('ku', 'ka'), Current merges: 557\n",
            "Merging: ('ya', 'fe'), Current merges: 558\n",
            "Merging: ('ro', 'na'), Current merges: 559\n",
            "Merging: ('za', 'ta'), Current merges: 560\n",
            "Merging: ('n', 'una'), Current merges: 561\n",
            "Merging: ('ta', 're'), Current merges: 562\n",
            "Merging: ('B', 'an'), Current merges: 563\n",
            "Merging: ('t', 'ana'), Current merges: 564\n",
            "Merging: ('Ba', 'ba'), Current merges: 565\n",
            "Merging: ('ba', 'ri'), Current merges: 566\n",
            "Merging: ('ma', 'fi'), Current merges: 567\n",
            "Merging: ('ba', 'ku'), Current merges: 568\n",
            "Merging: ('Du', 'k'), Current merges: 569\n",
            "Merging: ('ga', 're'), Current merges: 570\n",
            "Merging: ('D', 'A'), Current merges: 571\n",
            "Merging: ('ga', 'far'), Current merges: 572\n",
            "Merging: ('us', 'he'), Current merges: 573\n",
            "Merging: ('tun', 'da'), Current merges: 574\n",
            "Merging: ('do', 'le'), Current merges: 575\n",
            "Merging: ('magana', 'r'), Current merges: 576\n",
            "Merging: ('mi', 'ki'), Current merges: 577\n",
            "Merging: ('K', 'e'), Current merges: 578\n",
            "Merging: ('Wallah', 'i'), Current merges: 579\n",
            "Merging: ('ba', 'h'), Current merges: 580\n",
            "Merging: ('abun', 'da'), Current merges: 581\n",
            "Merging: ('jan', 'na'), Current merges: 582\n",
            "Merging: ('W', 'lh'), Current merges: 583\n",
            "Merging: ('ti', 'on'), Current merges: 584\n",
            "Merging: ('ai', 'ki'), Current merges: 585\n",
            "Merging: ('B', 'A'), Current merges: 586\n",
            "Merging: ('😍', '😍'), Current merges: 587\n",
            "Merging: ('la', 'la'), Current merges: 588\n",
            "Merging: ('ka', 'wa'), Current merges: 589\n",
            "Merging: ('an', 'yi'), Current merges: 590\n",
            "Merging: ('in', 'da'), Current merges: 591\n",
            "Merging: ('ta', 'ya'), Current merges: 592\n",
            "Merging: ('i', 'ki'), Current merges: 593\n",
            "Merging: ('am', 'een'), Current merges: 594\n",
            "Merging: ('j', 'en'), Current merges: 595\n",
            "Merging: ('san', 'i'), Current merges: 596\n",
            "Merging: ('k', 'wan'), Current merges: 597\n",
            "Merging: ('ra', 'ji'), Current merges: 598\n",
            "Merging: ('so', 'sai'), Current merges: 599\n",
            "Merging: ('n', 'in'), Current merges: 600\n",
            "Merging: ('am', 'fan'), Current merges: 601\n",
            "Merging: ('g', 'un'), Current merges: 602\n",
            "Merging: ('ya', 'ce'), Current merges: 603\n",
            "Merging: ('ll', 'o'), Current merges: 604\n",
            "Merging: ('ba', 'sira'), Current merges: 605\n",
            "Merging: ('l', 'f'), Current merges: 606\n",
            "Merging: ('da', 'su'), Current merges: 607\n",
            "Merging: ('ma', 'ganin'), Current merges: 608\n",
            "Merging: ('ka', 'ji'), Current merges: 609\n",
            "Merging: ('kun', 'ya'), Current merges: 610\n",
            "Merging: ('da', 'din'), Current merges: 611\n",
            "Merging: ('mul', 'kin'), Current merges: 612\n",
            "Merging: ('m', 'y'), Current merges: 613\n",
            "Merging: ('S', 'A'), Current merges: 614\n",
            "Merging: ('p', 'o'), Current merges: 615\n",
            "Merging: ('za', 'ku'), Current merges: 616\n",
            "Merging: ('ha', 'ku'), Current merges: 617\n",
            "Merging: ('p', 'e'), Current merges: 618\n",
            "Merging: ('p', 'a'), Current merges: 619\n",
            "Merging: ('i', 'man'), Current merges: 620\n",
            "Merging: ('lla', 'i'), Current merges: 621\n",
            "Merging: ('👍', '👍'), Current merges: 622\n",
            "Merging: ('gafar', 'ta'), Current merges: 623\n",
            "Merging: ('da', 'ke'), Current merges: 624\n",
            "Merging: ('S', 'ar'), Current merges: 625\n",
            "Merging: ('🙏🙏', '🙏🙏'), Current merges: 626\n",
            "Merging: ('wa', 'hala'), Current merges: 627\n",
            "Merging: ('c', 'h'), Current merges: 628\n",
            "Merging: ('Wan', 'da'), Current merges: 629\n",
            "Merging: ('hi', 's'), Current merges: 630\n",
            "Merging: ('lai', 'hi'), Current merges: 631\n",
            "Merging: ('ku', 'll'), Current merges: 632\n",
            "Merging: ('al', 'khairi'), Current merges: 633\n",
            "Merging: ('za', \"'a\"), Current merges: 634\n",
            "Merging: ('Allah', 'u'), Current merges: 635\n",
            "Merging: ('un', 'ci'), Current merges: 636\n",
            "Merging: ('ab', 'i'), Current merges: 637\n",
            "Merging: ('to', 'h'), Current merges: 638\n",
            "Merging: ('S', 'he'), Current merges: 639\n",
            "Merging: ('Ka', 'ra'), Current merges: 640\n",
            "Merging: ('Inna', 'lillahi'), Current merges: 641\n",
            "Merging: ('ƙ', 'a'), Current merges: 642\n",
            "Merging: ('ku', 'di'), Current merges: 643\n",
            "Merging: ('gu', 'du'), Current merges: 644\n",
            "Merging: ('D', 'o'), Current merges: 645\n",
            "Merging: ('!!', '!'), Current merges: 646\n",
            "Merging: ('s', 'iya'), Current merges: 647\n",
            "Merging: ('ta', 'fi'), Current merges: 648\n",
            "Merging: ('b', 'b'), Current merges: 649\n",
            "Merging: ('Da', 'ma'), Current merges: 650\n",
            "Merging: ('A', 'm'), Current merges: 651\n",
            "Merging: ('Ha', 'usa'), Current merges: 652\n",
            "Merging: ('..', '...'), Current merges: 653\n",
            "Merging: ('ba', 'b'), Current merges: 654\n",
            "Merging: ('Kan', 'o'), Current merges: 655\n",
            "Merging: ('w', 'llh'), Current merges: 656\n",
            "Merging: ('fa', 'h'), Current merges: 657\n",
            "Merging: ('g', 'in'), Current merges: 658\n",
            "Merging: ('s', 'ke'), Current merges: 659\n",
            "Merging: ('s', 'sa'), Current merges: 660\n",
            "Merging: ('ga', 'r'), Current merges: 661\n",
            "Merging: ('sh', 'an'), Current merges: 662\n",
            "Merging: ('tai', 'maka'), Current merges: 663\n",
            "Merging: ('ski', 'a'), Current merges: 664\n",
            "Merging: ('M', 'ana'), Current merges: 665\n",
            "Merging: ('ta', 'm'), Current merges: 666\n",
            "Merging: ('k', 'ya'), Current merges: 667\n",
            "Merging: ('Allah', ','), Current merges: 668\n",
            "Merging: ('j', 'o'), Current merges: 669\n",
            "Merging: ('wa', 'dan'), Current merges: 670\n",
            "Merging: ('ba', 'ra'), Current merges: 671\n",
            "Merging: ('a', 'la'), Current merges: 672\n",
            "Merging: (\"'\", 'i'), Current merges: 673\n",
            "Merging: ('o', 'o'), Current merges: 674\n",
            "Merging: ('ka', 'san'), Current merges: 675\n",
            "Merging: ('ci', 'gaba'), Current merges: 676\n",
            "Merging: ('a', 'c'), Current merges: 677\n",
            "Merging: ('du', 'je'), Current merges: 678\n",
            "Merging: ('lf', 'y'), Current merges: 679\n",
            "Merging: ('ha', 't'), Current merges: 680\n",
            "Merging: ('an', 'd'), Current merges: 681\n",
            "Merging: ('A', 'rewa'), Current merges: 682\n",
            "Merging: ('wa', 'jen'), Current merges: 683\n",
            "Merging: ('u', 'ba'), Current merges: 684\n",
            "Merging: ('e', 'e'), Current merges: 685\n",
            "Merging: ('Ka', 'ji'), Current merges: 686\n",
            "Merging: ('kan', 'o'), Current merges: 687\n",
            "Merging: ('sau', 'ran'), Current merges: 688\n",
            "Merging: ('k', 'wana'), Current merges: 689\n",
            "Merging: ('Uban', 'giji'), Current merges: 690\n",
            "Merging: ('d', 'iya'), Current merges: 691\n",
            "Merging: ('talaka', 'wa'), Current merges: 692\n",
            "Merging: ('ra', 'sa'), Current merges: 693\n",
            "Merging: ('K', 'A'), Current merges: 694\n",
            "Merging: ('mu', 'tun'), Current merges: 695\n",
            "Merging: ('ha', 'lin'), Current merges: 696\n",
            "Merging: ('loka', 'cin'), Current merges: 697\n",
            "Merging: ('ra', 'm'), Current merges: 698\n",
            "Merging: ('n', 'n'), Current merges: 699\n",
            "Merging: ('S', 'o'), Current merges: 700\n",
            "Merging: ('b', 'hana'), Current merges: 701\n",
            "Merging: ('dau', 'ki'), Current merges: 702\n",
            "Merging: ('lo', 've'), Current merges: 703\n",
            "Merging: ('is', 'ka'), Current merges: 704\n",
            "Merging: ('ha', 'li'), Current merges: 705\n",
            "Merging: ('r', 'y'), Current merges: 706\n",
            "Merging: ('hau', 'ka'), Current merges: 707\n",
            "Merging: ('v', 'i'), Current merges: 708\n",
            "Merging: ('wu', 'ta'), Current merges: 709\n",
            "Merging: ('ad', 'din'), Current merges: 710\n",
            "Merging: ('J', 'a'), Current merges: 711\n",
            "Merging: ('ha', 'usa'), Current merges: 712\n",
            "Merging: ('i', 'kon'), Current merges: 713\n",
            "Merging: ('mutan', 'en'), Current merges: 714\n",
            "Merging: ('t', 'on'), Current merges: 715\n",
            "Merging: ('i', 'laihi'), Current merges: 716\n",
            "Merging: ('ha', 'mma'), Current merges: 717\n",
            "Merging: ('amfan', 'i'), Current merges: 718\n",
            "Merging: ('ta', 'bba'), Current merges: 719\n",
            "Merging: ('C', 'o'), Current merges: 720\n",
            "Merging: ('hu', 'ta'), Current merges: 721\n",
            "Merging: ('z', 'za'), Current merges: 722\n",
            "Merging: ('an', 'an'), Current merges: 723\n",
            "Merging: ('i', 'i'), Current merges: 724\n",
            "Merging: ('j', 'ar'), Current merges: 725\n",
            "Merging: ('K', 'i'), Current merges: 726\n",
            "Merging: ('ham', 'du'), Current merges: 727\n",
            "Merging: ('ma', 'tu'), Current merges: 728\n",
            "Merging: ('kiya', 'ye'), Current merges: 729\n",
            "Merging: ('ka', 'ga'), Current merges: 730\n",
            "Merging: ('💯💯', '💯💯'), Current merges: 731\n",
            "Merging: ('A', 'b'), Current merges: 732\n",
            "Merging: ('ta', 'usa'), Current merges: 733\n",
            "Merging: ('M', 'A'), Current merges: 734\n",
            "Merging: ('hh', 'hh'), Current merges: 735\n",
            "Merging: ('🚶', '🚶'), Current merges: 736\n",
            "Merging: ('😥', '😥'), Current merges: 737\n",
            "Merging: ('bi', 'r'), Current merges: 738\n",
            "Merging: ('S', 'an'), Current merges: 739\n",
            "Merging: ('g', 'iya'), Current merges: 740\n",
            "Merging: ('ya', 'ji'), Current merges: 741\n",
            "Merging: ('ha', 'da'), Current merges: 742\n",
            "Merging: ('tsa', 'ro'), Current merges: 743\n",
            "Merging: ('ne', 'man'), Current merges: 744\n",
            "Merging: ('ma', 'si'), Current merges: 745\n",
            "Merging: ('G', 'o'), Current merges: 746\n",
            "Merging: ('🔥', '🔥'), Current merges: 747\n",
            "Merging: ('da', 'y'), Current merges: 748\n",
            "Merging: (\"'\", 'yan'), Current merges: 749\n",
            "Merging: ('q', 'an'), Current merges: 750\n",
            "Merging: ('la', 'barin'), Current merges: 751\n",
            "Merging: ('am', 'an'), Current merges: 752\n",
            "Merging: ('ban', 'i'), Current merges: 753\n",
            "Merging: ('W', 'A'), Current merges: 754\n",
            "Merging: ('A', 'bin'), Current merges: 755\n",
            "Merging: ('sh', 'ar'), Current merges: 756\n",
            "Merging: ('ya', 'kawo'), Current merges: 757\n",
            "Merging: ('j', 'am'), Current merges: 758\n",
            "Merging: ('mu', 'ku'), Current merges: 759\n",
            "Merging: ('hi', 'ra'), Current merges: 760\n",
            "Merging: ('yan', 'e'), Current merges: 761\n",
            "Merging: ('S', 'abo'), Current merges: 762\n",
            "Merging: ('na', 'ke'), Current merges: 763\n",
            "Merging: ('s', 'ta'), Current merges: 764\n",
            "Merging: ('en', 't'), Current merges: 765\n",
            "Merging: ('🙏', '🏼'), Current merges: 766\n",
            "Merging: ('a', 'si'), Current merges: 767\n",
            "Merging: ('ja', 'ma'), Current merges: 768\n",
            "Merging: ('Na', 'jeriya'), Current merges: 769\n",
            "Merging: ('yan', 'xu'), Current merges: 770\n",
            "Merging: ('w', 'on'), Current merges: 771\n",
            "Merging: ('is', 'kan'), Current merges: 772\n",
            "Merging: ('haku', 'ri'), Current merges: 773\n",
            "Merging: ('🤣🤣', '🤣🤣'), Current merges: 774\n",
            "Merging: ('🤲🤲', '🤲'), Current merges: 775\n",
            "Merging: ('An', 'nabi'), Current merges: 776\n",
            "Merging: ('🏃', '🏃'), Current merges: 777\n",
            "Merging: ('M', 'e'), Current merges: 778\n",
            "Merging: ('ba', 'za'), Current merges: 779\n",
            "Merging: ('ka', 'u'), Current merges: 780\n",
            "Merging: ('B', 'abu'), Current merges: 781\n",
            "Merging: ('A', 'bu'), Current merges: 782\n",
            "Merging: ('G', 'wa'), Current merges: 783\n",
            "Merging: ('su', 'ma'), Current merges: 784\n",
            "Merging: ('G', 'an'), Current merges: 785\n",
            "Merging: ('li', 'llah'), Current merges: 786\n",
            "Merging: ('musul', 'mi'), Current merges: 787\n",
            "Merging: ('Ra', 'hma'), Current merges: 788\n",
            "Merging: ('Al', 'hamdu'), Current merges: 789\n",
            "Merging: ('am', 'man'), Current merges: 790\n",
            "Merging: ('ra', 'ba'), Current merges: 791\n",
            "Merging: ('da', 'de'), Current merges: 792\n",
            "Merging: ('ki', 'ke'), Current merges: 793\n",
            "Merging: ('l', 'ci'), Current merges: 794\n",
            "Merging: ('Sabo', 'da'), Current merges: 795\n",
            "Merging: ('ray', 'uwa'), Current merges: 796\n",
            "Merging: ('ce', 'n'), Current merges: 797\n",
            "Merging: ('yan', 'da'), Current merges: 798\n",
            "Merging: ('mu', ','), Current merges: 799\n",
            "Merging: ('c', 'he'), Current merges: 800\n",
            "Merging: ('tso', 'ron'), Current merges: 801\n",
            "Merging: ('ma', 'lam'), Current merges: 802\n",
            "Merging: ('re', 'c'), Current merges: 803\n",
            "Merging: ('an', 'nabi'), Current merges: 804\n",
            "Merging: ('💃', '💃'), Current merges: 805\n",
            "Merging: ('wa', 'ye'), Current merges: 806\n",
            "Merging: ('mur', 'na'), Current merges: 807\n",
            "Merging: ('r', 'anar'), Current merges: 808\n",
            "Merging: ('ta', \"'a\"), Current merges: 809\n",
            "Merging: ('wan', 'e'), Current merges: 810\n",
            "Merging: ('j', 'u'), Current merges: 811\n",
            "Merging: ('iman', 'i'), Current merges: 812\n",
            "Merging: ('mul', 'ki'), Current merges: 813\n",
            "Merging: ('N', 'A'), Current merges: 814\n",
            "Merging: ('ne', ','), Current merges: 815\n",
            "Merging: ('kan', 'in'), Current merges: 816\n",
            "Merging: ('mu', 'ka'), Current merges: 817\n",
            "Merging: ('ko', 'h'), Current merges: 818\n",
            "Merging: ('ne', 'h'), Current merges: 819\n",
            "Merging: ('yar', 'da'), Current merges: 820\n",
            "Merging: ('c', 'iya'), Current merges: 821\n",
            "Merging: ('y', 'yu'), Current merges: 822\n",
            "Merging: ('Ya', 'yi'), Current merges: 823\n",
            "Merging: ('Su', 'bhana'), Current merges: 824\n",
            "Merging: ('t', 'su'), Current merges: 825\n",
            "Merging: ('A', 'kwai'), Current merges: 826\n",
            "Merging: (',', ','), Current merges: 827\n",
            "Merging: ('kar', 'an'), Current merges: 828\n",
            "Merging: ('Y', 'ar'), Current merges: 829\n",
            "Merging: ('g', 'yara'), Current merges: 830\n",
            "Merging: ('S', 'un'), Current merges: 831\n",
            "Merging: ('M', 'un'), Current merges: 832\n",
            "Merging: ('k', 'us'), Current merges: 833\n",
            "Merging: ('da', 'e'), Current merges: 834\n",
            "Merging: ('na', 'mu'), Current merges: 835\n",
            "Merging: ('t', 'ta'), Current merges: 836\n",
            "Merging: ('sa', 'dau'), Current merges: 837\n",
            "Merging: ('Mu', 'hamma'), Current merges: 838\n",
            "Merging: ('so', 'ya'), Current merges: 839\n",
            "Merging: ('💪', '💪'), Current merges: 840\n",
            "Merging: ('d', 'ana'), Current merges: 841\n",
            "Merging: ('ka', 'dai'), Current merges: 842\n",
            "Merging: ('she', 'kara'), Current merges: 843\n",
            "Merging: ('S', 'ha'), Current merges: 844\n",
            "Merging: ('san', 'nan'), Current merges: 845\n",
            "Merging: ('dai', 'dai'), Current merges: 846\n",
            "Merging: ('mu', 'yi'), Current merges: 847\n",
            "Merging: ('p', 'le'), Current merges: 848\n",
            "Merging: ('😄', '😄'), Current merges: 849\n",
            "Merging: ('sa', 'llah'), Current merges: 850\n",
            "Merging: ('Y', 'ana'), Current merges: 851\n",
            "Merging: ('to', 'r'), Current merges: 852\n",
            "Merging: ('go', 'diya'), Current merges: 853\n",
            "Merging: ('🙏', '🏽'), Current merges: 854\n",
            "Merging: ('ad', 'du'), Current merges: 855\n",
            "Merging: ('ra', 'in'), Current merges: 856\n",
            "Merging: ('gan', 'e'), Current merges: 857\n",
            "Merging: ('tu', 'nan'), Current merges: 858\n",
            "Merging: ('q', 'i'), Current merges: 859\n",
            "Merging: ('c', 'han'), Current merges: 860\n",
            "Merging: ('mutu', 'wa'), Current merges: 861\n",
            "Merging: ('H', 'A'), Current merges: 862\n",
            "Merging: ('tu', 'ba'), Current merges: 863\n",
            "Merging: ('shi', 'rin'), Current merges: 864\n",
            "Merging: ('al', 'i'), Current merges: 865\n",
            "Merging: ('o', 'u'), Current merges: 866\n",
            "Merging: ('ma', 'ri'), Current merges: 867\n",
            "Merging: ('bu', 'r'), Current merges: 868\n",
            "Merging: ('na', 'ka'), Current merges: 869\n",
            "Merging: ('shir', 'ya'), Current merges: 870\n",
            "Merging: ('Ma', 'lam'), Current merges: 871\n",
            "Merging: ('ta', 'mu'), Current merges: 872\n",
            "Merging: ('🇳🇬', '🇳🇬'), Current merges: 873\n",
            "Merging: ('ni', 'geria'), Current merges: 874\n",
            "Merging: ('y', 'un'), Current merges: 875\n",
            "Merging: ('🤔🤔', '🤔'), Current merges: 876\n",
            "Merging: ('hi', 'm'), Current merges: 877\n",
            "Merging: ('ra', 'ina'), Current merges: 878\n",
            "Merging: ('na', 'wa'), Current merges: 879\n",
            "Merging: ('ya', 'da'), Current merges: 880\n",
            "Merging: ('S', 'au'), Current merges: 881\n",
            "Merging: ('T', 'ab'), Current merges: 882\n",
            "Merging: ('in', 'sha'), Current merges: 883\n",
            "Merging: ('tso', 'h'), Current merges: 884\n",
            "Merging: ('ba', \"'a\"), Current merges: 885\n",
            "Merging: ('gir', 'ma'), Current merges: 886\n",
            "Merging: ('du', 'na'), Current merges: 887\n",
            "Merging: ('yi', 'wa'), Current merges: 888\n",
            "Merging: ('du', 'ba'), Current merges: 889\n",
            "Merging: ('ka', 'ke'), Current merges: 890\n",
            "Merging: ('b', 'd'), Current merges: 891\n",
            "Merging: ('t', 'his'), Current merges: 892\n",
            "Merging: ('ya', 'kamata'), Current merges: 893\n",
            "Merging: ('p', 'y'), Current merges: 894\n",
            "Merging: ('al', 'janna'), Current merges: 895\n",
            "Merging: ('ts', 'e'), Current merges: 896\n",
            "Merging: ('a', 're'), Current merges: 897\n",
            "Merging: ('ya', 'san'), Current merges: 898\n",
            "Merging: ('Ya', 'u'), Current merges: 899\n",
            "Merging: ('ta', 'shi'), Current merges: 900\n",
            "Merging: ('or', 'th'), Current merges: 901\n",
            "Merging: (\"'\", 's'), Current merges: 902\n",
            "Merging: ('a', 't'), Current merges: 903\n",
            "Merging: ('h', 'er'), Current merges: 904\n",
            "Merging: ('ll', 'e'), Current merges: 905\n",
            "Merging: ('g', 'sky'), Current merges: 906\n",
            "Merging: ('bi', 'yu'), Current merges: 907\n",
            "Merging: ('da', 'ina'), Current merges: 908\n",
            "Merging: ('A', 'she'), Current merges: 909\n",
            "Merging: ('kan', 'su'), Current merges: 910\n",
            "Merging: ('fa', 'di'), Current merges: 911\n",
            "Merging: ('sa', ','), Current merges: 912\n",
            "Merging: ('ba', 'la'), Current merges: 913\n",
            "Merging: ('👌', '👌'), Current merges: 914\n",
            "Merging: ('D', 'on'), Current merges: 915\n",
            "Merging: ('Sar', 'ki'), Current merges: 916\n",
            "Merging: ('ya', 'jikan'), Current merges: 917\n",
            "Merging: ('N', 'an'), Current merges: 918\n",
            "Merging: ('wan', 'an'), Current merges: 919\n",
            "Merging: ('han', 'nu'), Current merges: 920\n",
            "Merging: ('z', 'e'), Current merges: 921\n",
            "Merging: ('su', 'yi'), Current merges: 922\n",
            "Merging: ('ki', 'ka'), Current merges: 923\n",
            "Merging: (\"'\", 'un'), Current merges: 924\n",
            "Merging: ('ts', 'iya'), Current merges: 925\n",
            "Merging: ('t', 'ter'), Current merges: 926\n",
            "Merging: ('Ha', 'ba'), Current merges: 927\n",
            "Merging: ('matsala', 'r'), Current merges: 928\n",
            "Merging: ('sa', 'ke'), Current merges: 929\n",
            "Merging: ('ra', 'hma'), Current merges: 930\n",
            "Merging: ('g', 'h'), Current merges: 931\n",
            "Merging: ('D', 'i'), Current merges: 932\n",
            "Merging: ('n', 'un'), Current merges: 933\n",
            "Merging: ('fi', 'to'), Current merges: 934\n",
            "Merging: ('sa', 'mun'), Current merges: 935\n",
            "Merging: ('al', 'l'), Current merges: 936\n",
            "Merging: ('ne', '.'), Current merges: 937\n",
            "Merging: ('kull', 'um'), Current merges: 938\n",
            "Merging: ('ka', 'da'), Current merges: 939\n",
            "Merging: ('l', 's'), Current merges: 940\n",
            "Merging: ('ba', 'ma'), Current merges: 941\n",
            "Merging: ('za', 'ka'), Current merges: 942\n",
            "Merging: ('on', 'g'), Current merges: 943\n",
            "Merging: ('mu', 'ma'), Current merges: 944\n",
            "Merging: ('🙏', '🏾'), Current merges: 945\n",
            "Merging: ('kare', 'mu'), Current merges: 946\n",
            "Merging: ('a', 'h'), Current merges: 947\n",
            "Merging: ('na', 'ga'), Current merges: 948\n",
            "Merging: ('B', 'o'), Current merges: 949\n",
            "Merging: ('kin', 'g'), Current merges: 950\n",
            "Merging: ('musul', 'unci'), Current merges: 951\n",
            "Merging: ('ka', 'fin'), Current merges: 952\n",
            "Merging: ('ll', 'on'), Current merges: 953\n",
            "Merging: ('gen', 'i'), Current merges: 954\n",
            "Merging: ('au', 're'), Current merges: 955\n",
            "Merging: ('M', 'U'), Current merges: 956\n",
            "Merging: ('na', 'yi'), Current merges: 957\n",
            "Merging: ('M', 'us'), Current merges: 958\n",
            "Merging: (\"'\", 'u'), Current merges: 959\n",
            "Merging: ('e', 's'), Current merges: 960\n",
            "Merging: ('al', 'h'), Current merges: 961\n",
            "Merging: ('da', 'ban'), Current merges: 962\n",
            "Merging: ('L', 'o'), Current merges: 963\n",
            "Merging: ('E', 'n'), Current merges: 964\n",
            "Merging: ('r', 'ki'), Current merges: 965\n",
            "Merging: ('di', 'a'), Current merges: 966\n",
            "Merging: ('ta', 'b'), Current merges: 967\n",
            "Merging: ('tai', 'ma'), Current merges: 968\n",
            "Merging: ('lla', 'm'), Current merges: 969\n",
            "Merging: ('ra', 'wa'), Current merges: 970\n",
            "Merging: ('gwamna', 'ti'), Current merges: 971\n",
            "Merging: ('ra', 'b'), Current merges: 972\n",
            "Merging: ('ya', 'ci'), Current merges: 973\n",
            "Merging: ('i', 't'), Current merges: 974\n",
            "Merging: ('su', 'wa'), Current merges: 975\n",
            "Merging: ('musul', 'mai'), Current merges: 976\n",
            "Merging: ('k', 'ka'), Current merges: 977\n",
            "Merging: ('B', 'ar'), Current merges: 978\n",
            "Merging: ('c', 'hin'), Current merges: 979\n",
            "Merging: ('S', 'hu'), Current merges: 980\n",
            "Merging: ('i', 'ba'), Current merges: 981\n",
            "Merging: ('z', 'iki'), Current merges: 982\n",
            "Merging: ('K', 'in'), Current merges: 983\n",
            "Merging: ('l', 'li'), Current merges: 984\n",
            "Merging: ('ba', 'zai'), Current merges: 985\n",
            "Merging: (\"'\", 'an'), Current merges: 986\n",
            "Merging: ('wa', 'to'), Current merges: 987\n",
            "Merging: ('muna', 'fu'), Current merges: 988\n",
            "Merging: ('ta', '.'), Current merges: 989\n",
            "Merging: ('ma', 'la'), Current merges: 990\n",
            "Merging: ('Alhamdu', 'lillah'), Current merges: 991\n",
            "Merging: ('wa', 'dai'), Current merges: 992\n",
            "Merging: ('za', 'be'), Current merges: 993\n",
            "Merging: ('ya', 'ushe'), Current merges: 994\n",
            "Merging: ('sa', '.'), Current merges: 995\n",
            "Merging: ('D', 'un'), Current merges: 996\n",
            "Merging: ('sa', 'ce'), Current merges: 997\n",
            "Merging: ('shi', ','), Current merges: 998\n",
            "Merging: ('S', 'e'), Current merges: 999\n",
            "Merging: ('ya', 'kai'), Current merges: 1000\n",
            "Merges:  {('a', 'n'): 'an', ('e', 'r'): 'er', ('u', 's'): 'us', ('y', 'a'): 'ya', ('@', 'us'): '@us', ('@us', 'er'): '@user', ('d', 'a'): 'da', ('k', 'a'): 'ka', ('i', 'n'): 'in', ('m', 'a'): 'ma', ('l', 'a'): 'la', ('b', 'a'): 'ba', ('w', 'a'): 'wa', ('s', 'a'): 'sa', ('t', 'a'): 'ta', ('l', 'la'): 'lla', ('lla', 'h'): 'llah', ('m', 'u'): 'mu', ('s', 'h'): 'sh', ('A', 'llah'): 'Allah', ('n', 'a'): 'na', ('r', 'a'): 'ra', ('s', 'u'): 'su', ('k', 'u'): 'ku', ('h', 'a'): 'ha', ('g', 'a'): 'ga', ('k', 'i'): 'ki', ('\"', '\"'): '\"\"', ('an', 'a'): 'ana', ('k', 'e'): 'ke', ('z', 'a'): 'za', ('y', 'i'): 'yi', ('.', '.'): '..', ('n', 'an'): 'nan', ('w', 'an'): 'wan', ('i', 'ya'): 'iya', ('u', 'n'): 'un', ('f', 'a'): 'fa', ('c', 'i'): 'ci', ('sh', 'i'): 'shi', ('k', 'o'): 'ko', ('c', 'e'): 'ce', ('r', 'e'): 're', ('r', 'i'): 'ri', ('j', 'i'): 'ji', ('b', 'an'): 'ban', ('a', 'i'): 'ai', ('n', 'e'): 'ne', ('k', 'an'): 'kan', ('k', 'in'): 'kin', ('😂', '😂'): '😂😂', ('y', 'an'): 'yan', ('\"\"', '\"\"'): '\"\"\"\"', ('e', 'n'): 'en', ('o', 'n'): 'on', ('d', 'an'): 'dan', ('d', 'u'): 'du', ('a', 'b'): 'ab', ('f', 'i'): 'fi', ('da', 'i'): 'dai', ('ka', 'r'): 'kar', ('😭', '😭'): '😭😭', ('a', 'm'): 'am', ('t', 'u'): 'tu', ('a', 'r'): 'ar', ('a', 'l'): 'al', ('r', 'in'): 'rin', ('b', 'u'): 'bu', ('wa', 'i'): 'wai', ('sa', 'i'): 'sai', ('s', 'o'): 'so', ('wan', 'nan'): 'wannan', ('l', 'i'): 'li', ('m', 'an'): 'man', ('u', 'wa'): 'uwa', ('ku', 'ma'): 'kuma', ('h', 'i'): 'hi', ('d', 'in'): 'din', ('Y', 'a'): 'Ya', ('m', 'ana'): 'mana', ('m', 'e'): 'me', ('s', 'an'): 'san', ('sh', 'e'): 'she', ('t', 'o'): 'to', ('ma', 'i'): 'mai', ('g', 'i'): 'gi', ('d', 'i'): 'di', ('b', 'i'): 'bi', ('t', 'sa'): 'tsa', ('t', 'an'): 'tan', ('M', 'a'): 'Ma', ('w', 'o'): 'wo', ('sh', 'a'): 'sha', ('in', 'a'): 'ina', ('🙏', '🙏'): '🙏🙏', ('ya', 'u'): 'yau', ('ki', 'ya'): 'kiya', ('N', 'i'): 'Ni', ('ka', 'i'): 'kai', ('g', 'an'): 'gan', ('m', 'in'): 'min', ('l', 'l'): 'll', ('m', 'i'): 'mi', ('er', 'i'): 'eri', ('K', 'a'): 'Ka', ('ba', 'r'): 'bar', ('U', 'M'): 'UM', ('🤣', '🤣'): '🤣🤣', ('B', 'a'): 'Ba', ('\"\"\"\"', '\"\"\"\"'): '\"\"\"\"\"\"\"\"', ('d', 'o'): 'do', ('[', 'N'): '[N', ('[N', 'UM'): '[NUM', ('[NUM', ']'): '[NUM]', ('h', 'an'): 'han', ('s', 'i'): 'si', ('h', 'e'): 'he', ('sa', 'r'): 'sar', ('j', 'a'): 'ja', ('ka', 'ra'): 'kara', ('D', 'a'): 'Da', ('y', 'e'): 'ye', ('..', '.'): '...', ('g', 'o'): 'go', ('t', 'i'): 'ti', ('M', 'u'): 'Mu', ('ya', 'r'): 'yar', ('sh', 'in'): 'shin', ('ya', 'sa'): 'yasa', ('c', 'in'): 'cin', ('m', 'us'): 'mus', ('mu', 'n'): 'mun', ('l', 'h'): 'lh', ('s', 'kiya'): 'skiya', ('g', 'eri'): 'geri', ('ya', 'yi'): 'yayi', ('l', 'e'): 'le', ('ta', 'r'): 'tar', ('K', 'u'): 'Ku', (\"'\", 'a'): \"'a\", ('geri', 'a'): 'geria', ('ma', 'r'): 'mar', ('d', 'e'): 'de', ('k', 'yau'): 'kyau', ('n', 'i'): 'ni', ('H', 'a'): 'Ha', ('r', 'u'): 'ru', ('da', 'u'): 'dau', ('r', 'o'): 'ro', ('ku', 'n'): 'kun', ('a', 'llah'): 'allah', ('mu', 'na'): 'muna', ('W', 'an'): 'Wan', ('z', 'u'): 'zu', ('g', 'u'): 'gu', ('ha', 'r'): 'har', ('G', 'a'): 'Ga', ('y', 'in'): 'yin', ('Ni', 'geria'): 'Nigeria', ('su', 'n'): 'sun', ('m', 'ma'): 'mma', ('wan', 'i'): 'wani', ('za', 'i'): 'zai', ('du', 'k'): 'duk', ('ka', 'wai'): 'kawai', ('r', 'an'): 'ran', ('ka', 'wo'): 'kawo', ('mus', 'u'): 'musu', ('D', 'an'): 'Dan', ('y', 'o'): 'yo', ('b', 'e'): 'be', ('ma', 'ta'): 'mata', ('h', 'u'): 'hu', ('ga', 'ba'): 'gaba', ('T', 'o'): 'To', ('t', 's'): 'ts', ('k', 'wa'): 'kwa', ('ab', 'in'): 'abin', ('t', 'un'): 'tun', ('R', 'a'): 'Ra', ('ka', 'sa'): 'kasa', ('b', 'o'): 'bo', ('da', 'ya'): 'daya', ('w', 'lh'): 'wlh', ('l', 'o'): 'lo', ('I', 'n'): 'In', ('mu', 'tu'): 'mutu', ('i', 's'): 'is', ('😂😂', '😂'): '😂😂😂', ('ci', 'kin'): 'cikin', ('A', 'l'): 'Al', ('a', 'ka'): 'aka', ('ha', 'ri'): 'hari', ('ke', 'nan'): 'kenan', ('j', 'e'): 'je', ('ma', 'su'): 'masu', ('d', 'un'): 'dun', ('..', '..'): '....', ('ya', 'ke'): 'yake', ('ha', 'ka'): 'haka', ('llah', 'i'): 'llahi', ('g', 'e'): 'ge', ('us', 'a'): 'usa', ('w', 'u'): 'wu', ('m', 'm'): 'mm', ('fa', 'r'): 'far', ('U', 'R'): 'UR', ('sa', 'u'): 'sau', ('f', 'iya'): 'fiya', ('ha', 'ma'): 'hama', ('N', 'a'): 'Na', ('y', 'ana'): 'yana', ('wa', 'r'): 'war', ('h', 'h'): 'hh', ('q', 'a'): 'qa', ('s', 'e'): 'se', ('!', '!'): '!!', ('am', 'ma'): 'amma', ('[', 'UR'): '[UR', ('[UR', 'L'): '[URL', ('[URL', ']'): '[URL]', ('h', 'o'): 'ho', ('L', 'A'): 'LA', ('j', 'in'): 'jin', ('mu', 'tan'): 'mutan', ('f', 'e'): 'fe', ('Wan', 'nan'): 'Wannan', ('ta', 'la'): 'tala', ('ga', 'skiya'): 'gaskiya', ('l', 'in'): 'lin', ('a', 'ke'): 'ake', ('o', 'r'): 'or', ('W', 'a'): 'Wa', ('u', 'ban'): 'uban', ('i', 'dan'): 'idan', ('i', 'rin'): 'irin', ('su', 'ka'): 'suka', ('🇳', '🇬'): '🇳🇬', ('d', 'da'): 'dda', ('f', 'in'): 'fin', ('u', 'na'): 'una', ('ka', 'she'): 'kashe', ('z', 'an'): 'zan', ('ban', 'za'): 'banza', ('da', 'ga'): 'daga', ('uwa', 'r'): 'uwar', ('in', 'g'): 'ing', ('A', 'L'): 'AL', ('🤲', '🤲'): '🤲🤲', ('A', 'n'): 'An', ('fa', 'ra'): 'fara', ('K', 'ai'): 'Kai', ('ka', 're'): 'kare', ('ma', 'sa'): 'masa', ('g', 'ana'): 'gana', ('A', 'i'): 'Ai', ('s', 'on'): 'son', ('ga', 'ban'): 'gaban', ('😭😭', '😭'): '😭😭😭', ('Ku', 'ma'): 'Kuma', ('ab', 'un'): 'abun', ('ba', 'ki'): 'baki', ('t', 'h'): 'th', ('S', 'u'): 'Su', ('A', 'me'): 'Ame', ('LA', 'H'): 'LAH', ('ba', 'bu'): 'babu', ('k', 'wai'): 'kwai', ('wan', 'da'): 'wanda', ('iya', 'r'): 'iyar', ('da', 'ma'): 'dama', ('ba', 'su'): 'basu', ('Ga', 'skiya'): 'Gaskiya', ('s', 't'): 'st', ('ba', 'ya'): 'baya', ('z', 'o'): 'zo', ('b', 'in'): 'bin', ('a', 'u'): 'au', ('ba', 'mu'): 'bamu', ('🤔', '🤔'): '🤔🤔', ('su', 'ke'): 'suke', ('la', 'fiya'): 'lafiya', ('shin', 'e'): 'shine', ('\\u200d', '♂'): '\\u200d♂', ('sh', 'en'): 'shen', ('AL', 'LAH'): 'ALLAH', ('ta', 'i'): 'tai', ('sh', 'u'): 'shu', ('B', 'u'): 'Bu', ('ma', 'ka'): 'maka', ('K', 'o'): 'Ko', ('ba', 'i'): 'bai', ('\\u200d♂', '️'): '\\u200d♂️', ('re', 'wa'): 'rewa', ('j', 'an'): 'jan', ('ha', 'i'): 'hai', ('gan', 'in'): 'ganin', ('ab', 'u'): 'abu', ('t', 'so'): 'tso', ('s', 's'): 'ss', ('T', 'a'): 'Ta', ('I', 'na'): 'Ina', ('\"\"\"\"\"\"\"\"', '\"\"\"\"\"\"\"\"'): '\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"', ('ko', 'n'): 'kon', ('ma', 'gana'): 'magana', ('y', 'u'): 'yu', ('ka', 'sar'): 'kasar', ('yo', 'u'): 'you', ('Ame', 'en'): 'Ameen', ('m', 'o'): 'mo', ('dun', 'iya'): 'duniya', ('y', 'ya'): 'yya', ('d', 'on'): 'don', ('han', 'ka'): 'hanka', ('musu', 'l'): 'musul', ('la', 'i'): 'lai', ('A', 'mma'): 'Amma', ('su', 'na'): 'suna', ('mutan', 'e'): 'mutane', ('🙄', '🙄'): '🙄🙄', ('tala', 'ka'): 'talaka', ('bar', 'ka'): 'barka', ('a', 'd'): 'ad', ('S', 'ai'): 'Sai', ('ko', 'wa'): 'kowa', ('😡', '😡'): '😡😡', ('h', 'ana'): 'hana', ('kar', 'ya'): 'karya', ('f', 'an'): 'fan', ('v', 'e'): 've', ('wa', 'ta'): 'wata', ('gi', 'ji'): 'giji', ('ji', 'kan'): 'jikan', ('B', 'B'): 'BB', ('gan', 'i'): 'gani', ('ma', 'tsa'): 'matsa', ('sa', 'ka'): 'saka', ('💯', '💯'): '💯💯', ('t', 'e'): 'te', ('m', 'na'): 'mna', ('g', 'wa'): 'gwa', ('A', 'N'): 'AN', ('w', 'i'): 'wi', ('i', 'ta'): 'ita', ('ba', ','): 'ba,', ('L', 'a'): 'La', ('ce', 'wa'): 'cewa', ('Y', 'an'): 'Yan', ('x', 'a'): 'xa', ('f', 'u'): 'fu', ('M', 'ai'): 'Mai', ('kyau', 'ta'): 'kyauta', ('ban', 'e'): 'bane', ('ra', 'hama'): 'rahama', ('c', 'o'): 'co', ('shi', 'ga'): 'shiga', ('a', 'kan'): 'akan', ('l', 'u'): 'lu', ('BB', 'C'): 'BBC', ('😠', '😠'): '😠😠', ('ka', 'yi'): 'kayi', ('😏', '😏'): '😏😏', ('shi', 'r'): 'shir', ('yan', 'zu'): 'yanzu', ('n', 'u'): 'nu', ('mu', 'l'): 'mul', ('la', 'm'): 'lam', ('n', 'o'): 'no', ('A', 'R'): 'AR', ('sa', 'mu'): 'samu', ('ba', 'da'): 'bada', ('hanka', 'li'): 'hankali', ('dau', 'ka'): 'dauka', ('e', 'en'): 'een', ('ta', 'ba'): 'taba', ('j', 'er'): 'jer', ('t', 'y'): 'ty', ('ba', 'yan'): 'bayan', ('Z', 'a'): 'Za', ('c', 'hi'): 'chi', ('ka', 'mata'): 'kamata', ('ku', 'wa'): 'kuwa', ('Bu', 'hari'): 'Buhari', ('ci', 'ki'): 'ciki', ('ba', 'ta'): 'bata', ('sa', 'bo'): 'sabo', ('🙏🙏', '🙏'): '🙏🙏🙏', ('😭😭', '😭😭'): '😭😭😭😭', ('za', 'mu'): 'zamu', ('t', 'he'): 'the', ('a', 'kwai'): 'akwai', ('w', 'w'): 'ww', ('i', 'sa'): 'isa', ('To', 'h'): 'Toh', ('za', 'man'): 'zaman', ('Mu', 'na'): 'Muna', ('sar', 'ki'): 'sarki', ('S', 'a'): 'Sa', ('A', 'min'): 'Amin', ('c', 'an'): 'can', ('wa', 'llahi'): 'wallahi', ('c', 'u'): 'cu', ('do', 'min'): 'domin', ('fa', 'da'): 'fada', ('ha', 'kan'): 'hakan', ('ba', '.'): 'ba.', ('Ma', 'sha'): 'Masha', ('t', 'in'): 'tin', ('ka', 'm'): 'kam', ('za', 'su'): 'zasu', ('za', 'ma'): 'zama', ('sabo', 'da'): 'saboda', ('h', 'in'): 'hin', ('ya', 'wa'): 'yawa', ('D', 'u'): 'Du', ('S', 'hi'): 'Shi', ('o', 'd'): 'od', ('sa', 'ra'): 'sara', ('na', 'bi'): 'nabi', ('😂😂', '😂😂'): '😂😂😂😂', ('ha', 'u'): 'hau', ('da', 'mu'): 'damu', ('abin', 'da'): 'abinda', ('a', 'yi'): 'ayi', ('In', 'na'): 'Inna', ('W', 'ai'): 'Wai', ('ka', 'mar'): 'kamar', ('ra', 'i'): 'rai', ('l', 'm'): 'lm', ('k', 'ana'): 'kana', ('?', '?'): '??', ('😁', '😁'): '😁😁', ('w', 'e'): 'we', ('kar', 'shen'): 'karshen', ('U', 'ban'): 'Uban', ('Ra', 'hama'): 'Rahama', ('ba', 'ka'): 'baka', ('ba', 'rin'): 'barin', ('ra', 'shin'): 'rashin', ('da', 'di'): 'dadi', ('shu', 'gaban'): 'shugaban', ('li', 'llahi'): 'lillahi', ('k', 'hai'): 'khai', ('bu', 'hari'): 'buhari', ('❤', '️'): '❤️', ('mutu', 'm'): 'mutum', ('b', 'ba'): 'bba', ('i', 'r'): 'ir', ('ko', 'ma'): 'koma', ('ra', 'r'): 'rar', ('ta', 'yi'): 'tayi', ('I', 'N'): 'IN', ('h', 'ma'): 'hma', ('ku', 'din'): 'kudin', ('ba', 'shi'): 'bashi', ('gi', 'da'): 'gida', ('y', 'en'): 'yen', ('K', 'ar'): 'Kar', ('🤣🤣', '🤣'): '🤣🤣🤣', ('s', 'ki'): 'ski', ('mu', 'ke'): 'muke', ('ana', 'r'): 'anar', ('jer', 'iya'): 'jeriya', ('b', 'iya'): 'biya', ('👏', '👏'): '👏👏', ('ya', 'kara'): 'yakara', ('ko', 'mai'): 'komai', ('K', 'an'): 'Kan', ('F', 'a'): 'Fa', ('ha', 'la'): 'hala', ('in', 'e'): 'ine', ('ya', 'ra'): 'yara', ('t', 'er'): 'ter', ('😂', '🤣'): '😂🤣', ('ll', 'h'): 'llh', ('lo', 'ka'): 'loka', ('k', 'y'): 'ky', ('o', 'f'): 'of', ('ku', 'ke'): 'kuke', ('l', 'an'): 'lan', ('z', 'uwa'): 'zuwa', ('ra', 'y'): 'ray', ('ta', 'ke'): 'take', ('ya', 'fi'): 'yafi', ('si', 'ra'): 'sira', ('khai', 'ri'): 'khairi', ('mu', 'r'): 'mur', ('ai', 'kin'): 'aikin', ('ku', 'na'): 'kuna', ('a', 'da'): 'ada', ('Y', 'A'): 'YA', ('gi', 'r'): 'gir', ('da', 'shi'): 'dashi', ('😢', '😢'): '😢😢', ('r', 'uwa'): 'ruwa', ('l', 'y'): 'ly', ('p', 'p'): 'pp', ('ab', 'o'): 'abo', ('😎', '😎'): '😎😎', ('matsa', 'la'): 'matsala', ('u', 'm'): 'um', ('fa', 'tan'): 'fatan', ('p', 'i'): 'pi', ('ma', 'ki'): 'maki', ('u', 'r'): 'ur', ('j', 'iya'): 'jiya', ('re', 'n'): 'ren', ('v', 'er'): 'ver', ('ba', 'wa'): 'bawa', ('g', 'en'): 'gen', ('😅', '😅'): '😅😅', ('ka', 'ce'): 'kace', ('ya', 'dda'): 'yadda', ('ka', 'ma'): 'kama', ('a', 'ce'): 'ace', ('w', 'ana'): 'wana', ('ha', 'ha'): 'haha', ('ka', 'ta'): 'kata', ('f', 'or'): 'for', ('H', 'mm'): 'Hmm', ('ga', 'ri'): 'gari', ('al', 'barka'): 'albarka', ('q', 'ar'): 'qar', ('a', 'rewa'): 'arewa', ('r', 'on'): 'ron', ('🙏', '🏻'): '🙏🏻', ('ba', 'kin'): 'bakin', ('ba', 'ba'): 'baba', ('gi', 'dan'): 'gidan', ('A', 'na'): 'Ana', ('ci', 'ka'): 'cika', ('qa', 'ra'): 'qara', ('x', 'u'): 'xu', ('c', 'a'): 'ca', ('ga', 'ma'): 'gama', ('s', 'ky'): 'sky', ('am', 'in'): 'amin', ('ha', 'm'): 'ham', ('da', 'ra'): 'dara', ('kin', 'a'): 'kina', ('wa', 'su'): 'wasu', ('ru', 'wan'): 'ruwan', ('da', 'wo'): 'dawo', ('da', 'r'): 'dar', ('a', 'a'): 'aa', ('in', 'na'): 'inna', ('tsa', 're'): 'tsare', ('uban', 'giji'): 'ubangiji', ('gwa', 'mna'): 'gwamna', ('😀', '😀'): '😀😀', ('W', 'allah'): 'Wallah', ('s', 'in'): 'sin', ('da', 'ce'): 'dace', ('fi', 'lm'): 'film', ('I', 'dan'): 'Idan', ('dauka', 'ka'): 'daukaka', ('ku', 'ka'): 'kuka', ('ya', 'fe'): 'yafe', ('ro', 'na'): 'rona', ('za', 'ta'): 'zata', ('n', 'una'): 'nuna', ('ta', 're'): 'tare', ('B', 'an'): 'Ban', ('t', 'ana'): 'tana', ('Ba', 'ba'): 'Baba', ('ba', 'ri'): 'bari', ('ma', 'fi'): 'mafi', ('ba', 'ku'): 'baku', ('Du', 'k'): 'Duk', ('ga', 're'): 'gare', ('D', 'A'): 'DA', ('ga', 'far'): 'gafar', ('us', 'he'): 'ushe', ('tun', 'da'): 'tunda', ('do', 'le'): 'dole', ('magana', 'r'): 'maganar', ('mi', 'ki'): 'miki', ('K', 'e'): 'Ke', ('Wallah', 'i'): 'Wallahi', ('ba', 'h'): 'bah', ('abun', 'da'): 'abunda', ('jan', 'na'): 'janna', ('W', 'lh'): 'Wlh', ('ti', 'on'): 'tion', ('ai', 'ki'): 'aiki', ('B', 'A'): 'BA', ('😍', '😍'): '😍😍', ('la', 'la'): 'lala', ('ka', 'wa'): 'kawa', ('an', 'yi'): 'anyi', ('in', 'da'): 'inda', ('ta', 'ya'): 'taya', ('i', 'ki'): 'iki', ('am', 'een'): 'ameen', ('j', 'en'): 'jen', ('san', 'i'): 'sani', ('k', 'wan'): 'kwan', ('ra', 'ji'): 'raji', ('so', 'sai'): 'sosai', ('n', 'in'): 'nin', ('am', 'fan'): 'amfan', ('g', 'un'): 'gun', ('ya', 'ce'): 'yace', ('ll', 'o'): 'llo', ('ba', 'sira'): 'basira', ('l', 'f'): 'lf', ('da', 'su'): 'dasu', ('ma', 'ganin'): 'maganin', ('ka', 'ji'): 'kaji', ('kun', 'ya'): 'kunya', ('da', 'din'): 'dadin', ('mul', 'kin'): 'mulkin', ('m', 'y'): 'my', ('S', 'A'): 'SA', ('p', 'o'): 'po', ('za', 'ku'): 'zaku', ('ha', 'ku'): 'haku', ('p', 'e'): 'pe', ('p', 'a'): 'pa', ('i', 'man'): 'iman', ('lla', 'i'): 'llai', ('👍', '👍'): '👍👍', ('gafar', 'ta'): 'gafarta', ('da', 'ke'): 'dake', ('S', 'ar'): 'Sar', ('🙏🙏', '🙏🙏'): '🙏🙏🙏🙏', ('wa', 'hala'): 'wahala', ('c', 'h'): 'ch', ('Wan', 'da'): 'Wanda', ('hi', 's'): 'his', ('lai', 'hi'): 'laihi', ('ku', 'll'): 'kull', ('al', 'khairi'): 'alkhairi', ('za', \"'a\"): \"za'a\", ('Allah', 'u'): 'Allahu', ('un', 'ci'): 'unci', ('ab', 'i'): 'abi', ('to', 'h'): 'toh', ('S', 'he'): 'She', ('Ka', 'ra'): 'Kara', ('Inna', 'lillahi'): 'Innalillahi', ('ƙ', 'a'): 'ƙa', ('ku', 'di'): 'kudi', ('gu', 'du'): 'gudu', ('D', 'o'): 'Do', ('!!', '!'): '!!!', ('s', 'iya'): 'siya', ('ta', 'fi'): 'tafi', ('b', 'b'): 'bb', ('Da', 'ma'): 'Dama', ('A', 'm'): 'Am', ('Ha', 'usa'): 'Hausa', ('..', '...'): '.....', ('ba', 'b'): 'bab', ('Kan', 'o'): 'Kano', ('w', 'llh'): 'wllh', ('fa', 'h'): 'fah', ('g', 'in'): 'gin', ('s', 'ke'): 'ske', ('s', 'sa'): 'ssa', ('ga', 'r'): 'gar', ('sh', 'an'): 'shan', ('tai', 'maka'): 'taimaka', ('ski', 'a'): 'skia', ('M', 'ana'): 'Mana', ('ta', 'm'): 'tam', ('k', 'ya'): 'kya', ('Allah', ','): 'Allah,', ('j', 'o'): 'jo', ('wa', 'dan'): 'wadan', ('ba', 'ra'): 'bara', ('a', 'la'): 'ala', (\"'\", 'i'): \"'i\", ('o', 'o'): 'oo', ('ka', 'san'): 'kasan', ('ci', 'gaba'): 'cigaba', ('a', 'c'): 'ac', ('du', 'je'): 'duje', ('lf', 'y'): 'lfy', ('ha', 't'): 'hat', ('an', 'd'): 'and', ('A', 'rewa'): 'Arewa', ('wa', 'jen'): 'wajen', ('u', 'ba'): 'uba', ('e', 'e'): 'ee', ('Ka', 'ji'): 'Kaji', ('kan', 'o'): 'kano', ('sau', 'ran'): 'sauran', ('k', 'wana'): 'kwana', ('Uban', 'giji'): 'Ubangiji', ('d', 'iya'): 'diya', ('talaka', 'wa'): 'talakawa', ('ra', 'sa'): 'rasa', ('K', 'A'): 'KA', ('mu', 'tun'): 'mutun', ('ha', 'lin'): 'halin', ('loka', 'cin'): 'lokacin', ('ra', 'm'): 'ram', ('n', 'n'): 'nn', ('S', 'o'): 'So', ('b', 'hana'): 'bhana', ('dau', 'ki'): 'dauki', ('lo', 've'): 'love', ('is', 'ka'): 'iska', ('ha', 'li'): 'hali', ('r', 'y'): 'ry', ('hau', 'ka'): 'hauka', ('v', 'i'): 'vi', ('wu', 'ta'): 'wuta', ('ad', 'din'): 'addin', ('J', 'a'): 'Ja', ('ha', 'usa'): 'hausa', ('i', 'kon'): 'ikon', ('mutan', 'en'): 'mutanen', ('t', 'on'): 'ton', ('i', 'laihi'): 'ilaihi', ('ha', 'mma'): 'hamma', ('amfan', 'i'): 'amfani', ('ta', 'bba'): 'tabba', ('C', 'o'): 'Co', ('hu', 'ta'): 'huta', ('z', 'za'): 'zza', ('an', 'an'): 'anan', ('i', 'i'): 'ii', ('j', 'ar'): 'jar', ('K', 'i'): 'Ki', ('ham', 'du'): 'hamdu', ('ma', 'tu'): 'matu', ('kiya', 'ye'): 'kiyaye', ('ka', 'ga'): 'kaga', ('💯💯', '💯💯'): '💯💯💯💯', ('A', 'b'): 'Ab', ('ta', 'usa'): 'tausa', ('M', 'A'): 'MA', ('hh', 'hh'): 'hhhh', ('🚶', '🚶'): '🚶🚶', ('😥', '😥'): '😥😥', ('bi', 'r'): 'bir', ('S', 'an'): 'San', ('g', 'iya'): 'giya', ('ya', 'ji'): 'yaji', ('ha', 'da'): 'hada', ('tsa', 'ro'): 'tsaro', ('ne', 'man'): 'neman', ('ma', 'si'): 'masi', ('G', 'o'): 'Go', ('🔥', '🔥'): '🔥🔥', ('da', 'y'): 'day', (\"'\", 'yan'): \"'yan\", ('q', 'an'): 'qan', ('la', 'barin'): 'labarin', ('am', 'an'): 'aman', ('ban', 'i'): 'bani', ('W', 'A'): 'WA', ('A', 'bin'): 'Abin', ('sh', 'ar'): 'shar', ('ya', 'kawo'): 'yakawo', ('j', 'am'): 'jam', ('mu', 'ku'): 'muku', ('hi', 'ra'): 'hira', ('yan', 'e'): 'yane', ('S', 'abo'): 'Sabo', ('na', 'ke'): 'nake', ('s', 'ta'): 'sta', ('en', 't'): 'ent', ('🙏', '🏼'): '🙏🏼', ('a', 'si'): 'asi', ('ja', 'ma'): 'jama', ('Na', 'jeriya'): 'Najeriya', ('yan', 'xu'): 'yanxu', ('w', 'on'): 'won', ('is', 'kan'): 'iskan', ('haku', 'ri'): 'hakuri', ('🤣🤣', '🤣🤣'): '🤣🤣🤣🤣', ('🤲🤲', '🤲'): '🤲🤲🤲', ('An', 'nabi'): 'Annabi', ('🏃', '🏃'): '🏃🏃', ('M', 'e'): 'Me', ('ba', 'za'): 'baza', ('ka', 'u'): 'kau', ('B', 'abu'): 'Babu', ('A', 'bu'): 'Abu', ('G', 'wa'): 'Gwa', ('su', 'ma'): 'suma', ('G', 'an'): 'Gan', ('li', 'llah'): 'lillah', ('musul', 'mi'): 'musulmi', ('Ra', 'hma'): 'Rahma', ('Al', 'hamdu'): 'Alhamdu', ('am', 'man'): 'amman', ('ra', 'ba'): 'raba', ('da', 'de'): 'dade', ('ki', 'ke'): 'kike', ('l', 'ci'): 'lci', ('Sabo', 'da'): 'Saboda', ('ray', 'uwa'): 'rayuwa', ('ce', 'n'): 'cen', ('yan', 'da'): 'yanda', ('mu', ','): 'mu,', ('c', 'he'): 'che', ('tso', 'ron'): 'tsoron', ('ma', 'lam'): 'malam', ('re', 'c'): 'rec', ('an', 'nabi'): 'annabi', ('💃', '💃'): '💃💃', ('wa', 'ye'): 'waye', ('mur', 'na'): 'murna', ('r', 'anar'): 'ranar', ('ta', \"'a\"): \"ta'a\", ('wan', 'e'): 'wane', ('j', 'u'): 'ju', ('iman', 'i'): 'imani', ('mul', 'ki'): 'mulki', ('N', 'A'): 'NA', ('ne', ','): 'ne,', ('kan', 'in'): 'kanin', ('mu', 'ka'): 'muka', ('ko', 'h'): 'koh', ('ne', 'h'): 'neh', ('yar', 'da'): 'yarda', ('c', 'iya'): 'ciya', ('y', 'yu'): 'yyu', ('Ya', 'yi'): 'Yayi', ('Su', 'bhana'): 'Subhana', ('t', 'su'): 'tsu', ('A', 'kwai'): 'Akwai', (',', ','): ',,', ('kar', 'an'): 'karan', ('Y', 'ar'): 'Yar', ('g', 'yara'): 'gyara', ('S', 'un'): 'Sun', ('M', 'un'): 'Mun', ('k', 'us'): 'kus', ('da', 'e'): 'dae', ('na', 'mu'): 'namu', ('t', 'ta'): 'tta', ('sa', 'dau'): 'sadau', ('Mu', 'hamma'): 'Muhamma', ('so', 'ya'): 'soya', ('💪', '💪'): '💪💪', ('d', 'ana'): 'dana', ('ka', 'dai'): 'kadai', ('she', 'kara'): 'shekara', ('S', 'ha'): 'Sha', ('san', 'nan'): 'sannan', ('dai', 'dai'): 'daidai', ('mu', 'yi'): 'muyi', ('p', 'le'): 'ple', ('😄', '😄'): '😄😄', ('sa', 'llah'): 'sallah', ('Y', 'ana'): 'Yana', ('to', 'r'): 'tor', ('go', 'diya'): 'godiya', ('🙏', '🏽'): '🙏🏽', ('ad', 'du'): 'addu', ('ra', 'in'): 'rain', ('gan', 'e'): 'gane', ('tu', 'nan'): 'tunan', ('q', 'i'): 'qi', ('c', 'han'): 'chan', ('mutu', 'wa'): 'mutuwa', ('H', 'A'): 'HA', ('tu', 'ba'): 'tuba', ('shi', 'rin'): 'shirin', ('al', 'i'): 'ali', ('o', 'u'): 'ou', ('ma', 'ri'): 'mari', ('bu', 'r'): 'bur', ('na', 'ka'): 'naka', ('shir', 'ya'): 'shirya', ('Ma', 'lam'): 'Malam', ('ta', 'mu'): 'tamu', ('🇳🇬', '🇳🇬'): '🇳🇬🇳🇬', ('ni', 'geria'): 'nigeria', ('y', 'un'): 'yun', ('🤔🤔', '🤔'): '🤔🤔🤔', ('hi', 'm'): 'him', ('ra', 'ina'): 'raina', ('na', 'wa'): 'nawa', ('ya', 'da'): 'yada', ('S', 'au'): 'Sau', ('T', 'ab'): 'Tab', ('in', 'sha'): 'insha', ('tso', 'h'): 'tsoh', ('ba', \"'a\"): \"ba'a\", ('gir', 'ma'): 'girma', ('du', 'na'): 'duna', ('yi', 'wa'): 'yiwa', ('du', 'ba'): 'duba', ('ka', 'ke'): 'kake', ('b', 'd'): 'bd', ('t', 'his'): 'this', ('ya', 'kamata'): 'yakamata', ('p', 'y'): 'py', ('al', 'janna'): 'aljanna', ('ts', 'e'): 'tse', ('a', 're'): 'are', ('ya', 'san'): 'yasan', ('Ya', 'u'): 'Yau', ('ta', 'shi'): 'tashi', ('or', 'th'): 'orth', (\"'\", 's'): \"'s\", ('a', 't'): 'at', ('h', 'er'): 'her', ('ll', 'e'): 'lle', ('g', 'sky'): 'gsky', ('bi', 'yu'): 'biyu', ('da', 'ina'): 'daina', ('A', 'she'): 'Ashe', ('kan', 'su'): 'kansu', ('fa', 'di'): 'fadi', ('sa', ','): 'sa,', ('ba', 'la'): 'bala', ('👌', '👌'): '👌👌', ('D', 'on'): 'Don', ('Sar', 'ki'): 'Sarki', ('ya', 'jikan'): 'yajikan', ('N', 'an'): 'Nan', ('wan', 'an'): 'wanan', ('han', 'nu'): 'hannu', ('z', 'e'): 'ze', ('su', 'yi'): 'suyi', ('ki', 'ka'): 'kika', (\"'\", 'un'): \"'un\", ('ts', 'iya'): 'tsiya', ('t', 'ter'): 'tter', ('Ha', 'ba'): 'Haba', ('matsala', 'r'): 'matsalar', ('sa', 'ke'): 'sake', ('ra', 'hma'): 'rahma', ('g', 'h'): 'gh', ('D', 'i'): 'Di', ('n', 'un'): 'nun', ('fi', 'to'): 'fito', ('sa', 'mun'): 'samun', ('al', 'l'): 'all', ('ne', '.'): 'ne.', ('kull', 'um'): 'kullum', ('ka', 'da'): 'kada', ('l', 's'): 'ls', ('ba', 'ma'): 'bama', ('za', 'ka'): 'zaka', ('on', 'g'): 'ong', ('mu', 'ma'): 'muma', ('🙏', '🏾'): '🙏🏾', ('kare', 'mu'): 'karemu', ('a', 'h'): 'ah', ('na', 'ga'): 'naga', ('B', 'o'): 'Bo', ('kin', 'g'): 'king', ('musul', 'unci'): 'musulunci', ('ka', 'fin'): 'kafin', ('ll', 'on'): 'llon', ('gen', 'i'): 'geni', ('au', 're'): 'aure', ('M', 'U'): 'MU', ('na', 'yi'): 'nayi', ('M', 'us'): 'Mus', (\"'\", 'u'): \"'u\", ('e', 's'): 'es', ('al', 'h'): 'alh', ('da', 'ban'): 'daban', ('L', 'o'): 'Lo', ('E', 'n'): 'En', ('r', 'ki'): 'rki', ('di', 'a'): 'dia', ('ta', 'b'): 'tab', ('tai', 'ma'): 'taima', ('lla', 'm'): 'llam', ('ra', 'wa'): 'rawa', ('gwamna', 'ti'): 'gwamnati', ('ra', 'b'): 'rab', ('ya', 'ci'): 'yaci', ('i', 't'): 'it', ('su', 'wa'): 'suwa', ('musul', 'mai'): 'musulmai', ('k', 'ka'): 'kka', ('B', 'ar'): 'Bar', ('c', 'hin'): 'chin', ('S', 'hu'): 'Shu', ('i', 'ba'): 'iba', ('z', 'iki'): 'ziki', ('K', 'in'): 'Kin', ('l', 'li'): 'lli', ('ba', 'zai'): 'bazai', (\"'\", 'an'): \"'an\", ('wa', 'to'): 'wato', ('muna', 'fu'): 'munafu', ('ta', '.'): 'ta.', ('ma', 'la'): 'mala', ('Alhamdu', 'lillah'): 'Alhamdulillah', ('wa', 'dai'): 'wadai', ('za', 'be'): 'zabe', ('ya', 'ushe'): 'yaushe', ('sa', '.'): 'sa.', ('D', 'un'): 'Dun', ('sa', 'ce'): 'sace', ('shi', ','): 'shi,', ('S', 'e'): 'Se', ('ya', 'kai'): 'yakai'}\n",
            "BPE tokens:  ['T', 'his', 'is', 'a', 'te', 'st', 's', 'ent', 'en', 'ce', '.']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>bpe_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@user Da kudin da Arewa babu wani abin azo aga...</td>\n",
              "      <td>negative</td>\n",
              "      <td>@user Da kudin da Arewa babu wani abin a zo a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@user Kaga wani Adu ar Banda💔😭 wai a haka Shi ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>@user Ka ga wani A du ar Ban da 💔 😭 wai a haka...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@user Sai haquri fa yan madrid daman kunce cha...</td>\n",
              "      <td>negative</td>\n",
              "      <td>@user Sai ha q u ri fa yan ma d ri d da man ku...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@user Hmmm yanzu kai kasan girman allah daxaka...</td>\n",
              "      <td>negative</td>\n",
              "      <td>@user Hmm m yanzu kai kasan gir man allah da x...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@user @user Wai gwamno nin Nigeria suna afa kw...</td>\n",
              "      <td>negative</td>\n",
              "      <td>@user @user Wai gwa m no nin Nigeria suna a fa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text     label  \\\n",
              "0  @user Da kudin da Arewa babu wani abin azo aga...  negative   \n",
              "1  @user Kaga wani Adu ar Banda💔😭 wai a haka Shi ...  negative   \n",
              "2  @user Sai haquri fa yan madrid daman kunce cha...  negative   \n",
              "3  @user Hmmm yanzu kai kasan girman allah daxaka...  negative   \n",
              "4  @user @user Wai gwamno nin Nigeria suna afa kw...  negative   \n",
              "\n",
              "                                            bpe_text  \n",
              "0  @user Da kudin da Arewa babu wani abin a zo a ...  \n",
              "1  @user Ka ga wani A du ar Ban da 💔 😭 wai a haka...  \n",
              "2  @user Sai ha q u ri fa yan ma d ri d da man ku...  \n",
              "3  @user Hmm m yanzu kai kasan gir man allah da x...  \n",
              "4  @user @user Wai gwa m no nin Nigeria suna a fa...  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train BPE\n",
        "dummy = [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence']]\n",
        "bpe = BPETokenizer(tokenized_train_corpus, vocab_size=1000) # BPETokenizer(dummy, 20) #       \n",
        "merges = bpe.train()\n",
        "print('Merges: ', merges)\n",
        "\n",
        "# Tokenize text\n",
        "text = 'This is a test sentence.'\n",
        "tokenized_text = text.split()\n",
        "tokens = bpe.tokenize(text)\n",
        "print('BPE tokens: ', tokens)\n",
        "\n",
        "# Apply to our dataset\n",
        "train_df['bpe_text'] = train_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
        "dev_df['bpe_text'] = dev_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
        "test_df['bpe_text'] = test_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBAtehY1kGtZ"
      },
      "source": [
        "A vocabulary can then be created based on our BPE tokenised corpus. Specifying the ``vocab_size`` parameter of our BPE training algorithm allows us to control the vocabulary size, which enables smaller vocabularies than word-based tokenisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "h-rRXz_0Qyyo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of BPE tokens in corpus:  245125\n",
            "Number of BPE types in corpus:  1643\n",
            "@user: 11392\n",
            "da: 5864\n",
            "ya: 4894\n",
            "Allah: 4226\n",
            "a: 3680\n",
            "ka: 2976\n",
            "Vocabulary size:  1477\n",
            "First 10 BPE tokens in the vocabulary:  ['@user', 'da', 'ya', 'Allah', 'a', 'ka', 'ba', 'ta', 'ma', ',']\n"
          ]
        }
      ],
      "source": [
        "bpe_corpus = train_df['bpe_text'].tolist()\n",
        "tokenized_bpe_corpus = whitespace_tokenize(bpe_corpus)\n",
        "\n",
        "# Count number of BPE tokens in corpus\n",
        "num_tokens = count_tokens(tokenized_bpe_corpus)\n",
        "print('Number of BPE tokens in corpus: ', num_tokens)\n",
        "\n",
        "# Collect type counts in BPE corpus\n",
        "bpe_type2count = create_type_counts(tokenized_bpe_corpus)\n",
        "print('Number of BPE types in corpus: ', len(bpe_type2count))\n",
        "\n",
        "# Sort types by counts\n",
        "bpe_type2count = dict(sorted(bpe_type2count.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "# Print first few types and counts\n",
        "for i, (type_, count) in enumerate(bpe_type2count.items()):\n",
        "    print(f'{type_}: {count}')\n",
        "    if i == 5:\n",
        "        break\n",
        "\n",
        "# Create a vocabulary for BPE tokens\n",
        "bpe_index2type, bpe_type2index = create_vocabulary(bpe_type2count, min_count=2)\n",
        "\n",
        "# # Add unknown token to bpe\n",
        "# bpe_type2index['<UNK>'] = len(bpe_index2type)\n",
        "# bpe_index2type.append('<UNK>')\n",
        "\n",
        "print('Vocabulary size: ', len(bpe_index2type))\n",
        "print('First 10 BPE tokens in the vocabulary: ', bpe_index2type[0:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To improve readability, I set up separate dataframes for each tokenization method. These will be used when training the text classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BPE tokenization dataframe\n",
        "train_bpe = train_df[\"bpe_text\"].tolist()\n",
        "dev_bpe = dev_df[\"bpe_text\"].tolist()\n",
        "test_bpe = test_df[\"bpe_text\"].tolist()\n",
        "\n",
        "bpe_train_sentences = whitespace_tokenize(train_bpe)\n",
        "bpe_dev_sentences = whitespace_tokenize(dev_bpe)\n",
        "bpe_test_sentences = whitespace_tokenize(test_bpe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word tokenization dataframe\n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "dev_sentences = dev_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "\n",
        "tokenized_train_sentences = whitespace_tokenize(train_sentences)\n",
        "tokenized_dev_sentences = whitespace_tokenize(dev_sentences)\n",
        "tokenized_test_sentences = whitespace_tokenize(test_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxpu01RfQyyo"
      },
      "source": [
        "<a name=\"section2\"></a>\n",
        "# 2. Text Classification\n",
        "\n",
        "Now a text classification model can be trained for sentiment analysis on our data. The model task will be predicting the content as either positive or negative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWCXjTgUQyyo"
      },
      "source": [
        "<a name=\"section2_1\"></a>\n",
        "## 2.1. Text vectorization\n",
        "\n",
        "The textual data from the tweets must be transformed into a numerical format for the model to better understand and process. Vectorization is the process of converting text into numerical vectors. I opted for **One-hot encoding** of the data. Each sentence is represented as a binary vector with a 1 in the position corresponding to the word's index in the vocabulary and 0s elsewhere. Found below is a function to perform this vectorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Io87VrzFQyyo"
      },
      "outputs": [],
      "source": [
        "def one_hot_vectorize(sentences, type2index):\n",
        "    \"\"\"\n",
        "    One-hot encode a list of sentences.\n",
        "\n",
        "    param:\n",
        "        list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
        "        type2index: dictionary mapping words to their index in the vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
        "    return:\n",
        "        one_hot_sentences: 2d numpy array of one-hot encoded sentences e.g. [[1, 0, 0, 1, ...], [0, 1, 1, 0, ...], ...]\n",
        "    \"\"\"\n",
        "\n",
        "    one_hot_vectors = [] \n",
        "    for sentence in sentences:\n",
        "        vector = np.zeros(len(type2index))\n",
        "        for word in sentence:\n",
        "            if word in type2index:\n",
        "                vector[type2index[word]] = 1\n",
        "            else:\n",
        "                vector[type2index['<UNK>']] = 1\n",
        "            \n",
        "        one_hot_vectors.append(vector)\n",
        "\n",
        "    return one_hot_vectors\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the next block of code applies this vectorization to both tokenized versions of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "T58rP6R8Qyyp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BPE length:\n",
            "Train length:  9260\n",
            "Dev length:  1781\n",
            "Test length:  3514\n",
            "\n",
            "Word tokenization length:\n",
            "Train length:  9260\n",
            "Dev length:  1781\n",
            "Test length:  3514\n"
          ]
        }
      ],
      "source": [
        "# Vectorize dataframes\n",
        "X_train_word = one_hot_vectorize(tokenized_train_sentences, type2index)\n",
        "X_dev_word = one_hot_vectorize(tokenized_dev_sentences, type2index)\n",
        "X_test_word = one_hot_vectorize(tokenized_test_sentences, type2index)\n",
        "\n",
        "X_train_bpe = one_hot_vectorize(bpe_train_sentences, bpe_type2index)\n",
        "X_dev_bpe = one_hot_vectorize(bpe_dev_sentences, bpe_type2index)\n",
        "X_test_bpe = one_hot_vectorize(bpe_test_sentences, bpe_type2index)\n",
        "\n",
        "print(\"BPE length:\")\n",
        "print('Train length: ', len(X_train_bpe))\n",
        "print('Dev length: ', len(X_dev_bpe))\n",
        "print('Test length: ', len(X_test_bpe))\n",
        "\n",
        "print(\"\\nWord tokenization length:\")\n",
        "print('Train length: ', len(X_train_word))\n",
        "print('Dev length: ', len(X_dev_word))\n",
        "print('Test length: ', len(X_test_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEtSggW9Qyyp"
      },
      "source": [
        "We also convert the output labels to numerical representations (positive = 1 and negative = 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ozFLk-8EQyyp"
      },
      "outputs": [],
      "source": [
        "# Store as numpy array\n",
        "X_train = jnp.array(X_train_bpe, dtype=jnp.float16)\n",
        "X_dev = jnp.array(X_dev_bpe, dtype=jnp.float16)\n",
        "X_test = jnp.array(X_test_bpe, dtype=jnp.float16)\n",
        "n_feat = X_train.shape[1]\n",
        "\n",
        "# print('Train example:\\n', X_train[0:5])\n",
        "\n",
        "y_train = train_df[\"label\"]\n",
        "y_train = jnp.array(y_train.map({\"positive\": 1, \"negative\": 0}), dtype=jnp.float16)\n",
        "\n",
        "y_dev = dev_df[\"label\"]\n",
        "y_dev = jnp.array(y_dev.map({\"positive\": 1, \"negative\": 0}), dtype=jnp.float16)\n",
        "\n",
        "y_test = test_df[\"label\"]\n",
        "y_test = jnp.array(y_test.map({\"positive\": 1, \"negative\": 0}), dtype=jnp.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Naive Bayes Classifier\n",
        "\n",
        "A Naive bayes classifier uses conditional probability to predict the values associated with a set of features. It uses the probabilities of a distinct features being in a class to estimate the class of a set of features. As my first step, I calculated the frequency of each token in the two classes. These are stored in a frequency dict. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Frequency dict\n",
        "def frequency_dict(features, values):\n",
        "    freq_dict = defaultdict(int)\n",
        "    for sample_idx in range(len(features)):\n",
        "        for key_idx in range(len(features[sample_idx])):\n",
        "            present = features[sample_idx][key_idx].item()\n",
        "            if present:\n",
        "                freq_dict[(key_idx, values[sample_idx].item())]  += 1 \n",
        "    \n",
        "    return freq_dict\n",
        "\n",
        "# bpe_freq = frequency_dict(X_train_bpe, y_train)\n",
        "# word_freq = frequency_dict(X_train_word, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Frequencies are used to calculate the probability of eack token for the different sentiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model training\n",
        "def train_nb(train_x, train_y, alpha=1.0):\n",
        "    freqs = frequency_dict(train_x, train_y)\n",
        "    \n",
        "    loglikelihood = {}\n",
        "    logprior = {}\n",
        "\n",
        "    V = len(train_x[0])\n",
        "\n",
        "    # Total probabilities\n",
        "    N_pos = 0\n",
        "    N_neg = 0\n",
        "    for pair in freqs.keys():\n",
        "        if pair[1] > 0:\n",
        "            N_pos += freqs[(pair)]\n",
        "        else:\n",
        "            N_neg += freqs[(pair)]\n",
        "\n",
        "    # Total tweets\n",
        "    D = len(train_y)\n",
        "\n",
        "    # Positive documents\n",
        "    D_pos = sum(train_y)\n",
        "    \n",
        "    # Negative documents\n",
        "    D_neg = D - sum(train_y)\n",
        "\n",
        "    # Log prior\n",
        "    logprior = jnp.log(D_pos) - jnp.log(D_neg)\n",
        "\n",
        "    # Log likelihood per word\n",
        "    for idx in range(V):\n",
        "        # Frequency per label\n",
        "        freq_pos = freqs.get((idx, 1), 0)\n",
        "        freq_neg = freqs.get((idx, 0), 0)\n",
        "\n",
        "        # Probability with smoothing to avoid log of zero\n",
        "        p_pos = (freq_pos + alpha) / (N_pos +V)\n",
        "        p_neg = (freq_neg + alpha) / (N_neg +V)\n",
        "\n",
        "        # Log-likelihood\n",
        "        loglikelihood[idx] = jnp.log(p_pos / p_neg)\n",
        "\n",
        "    return logprior, loglikelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict sentiment as positive (1) or negative (0)\n",
        "def predict_nb(tweet, prior, likelihood):\n",
        "    p = prior\n",
        "    \n",
        "    for type_idx in range(len(tweet)):\n",
        "        if tweet[type_idx]:\n",
        "            p += likelihood[type_idx]\n",
        "    \n",
        "    # Positive prediction matches positive likelihood\n",
        "    return 1 if p > 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Evaluation\n",
        "\n",
        "The development dataset was used to evaluate model performance. This is done by predicting the output and using Scikit-learn's ```classification_report``` function. The first step uses Scikit's naive bayes classifier to make predictions. This was used during development as a performance target for my own implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word tokenisation benchmark:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.90      0.87       894\n",
            "         1.0       0.89      0.84      0.86       887\n",
            "\n",
            "    accuracy                           0.87      1781\n",
            "   macro avg       0.87      0.87      0.87      1781\n",
            "weighted avg       0.87      0.87      0.87      1781\n",
            "\n",
            "BPE tokenisation benchmark:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.92      0.88       894\n",
            "         1.0       0.91      0.82      0.86       887\n",
            "\n",
            "    accuracy                           0.87      1781\n",
            "   macro avg       0.87      0.87      0.87      1781\n",
            "weighted avg       0.87      0.87      0.87      1781\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import naive_bayes\n",
        "\n",
        "def benchmark_sklearn_nb(X_train, y_train, X_dev, y_dev):\n",
        "    \"\"\"\n",
        "    Trains a Naive Bayes classifier using scikit-learn and evaluates its performance.\n",
        "\n",
        "    Args:\n",
        "        X_train (list): Training data features.\n",
        "        y_train (jaxlib.xla_extension.ArrayImpl): Training data labels.\n",
        "        X_dev (list): Development data features.\n",
        "        y_dev (jaxlib.xla_extension.ArrayImpl): Development data labels.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    clf = naive_bayes.MultinomialNB()\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = clf.predict(X_dev)\n",
        "    y_pred = jnp.array(y_pred)\n",
        "    return classification_report(y_dev, y_pred)\n",
        "\n",
        "# Benchmark both tokenisation methods\n",
        "print(f\"Word tokenisation benchmark:\\n{benchmark_sklearn_nb(X_train_word, y_train, X_dev_word, y_dev)}\")\n",
        "print(f\"BPE tokenisation benchmark:\\n{benchmark_sklearn_nb(X_train_bpe, y_train, X_dev_bpe, y_dev)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Negative likelihood simplified to negative sentiment and positive likelihood to positive sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next block performs predictions over a set of input features and stores them in lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alpha: 0.1\n",
            "BPE tokenisation report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.96      0.87       894\n",
            "         1.0       0.94      0.75      0.84       887\n",
            "\n",
            "    accuracy                           0.85      1781\n",
            "   macro avg       0.87      0.85      0.85      1781\n",
            "weighted avg       0.87      0.85      0.85      1781\n",
            "\n",
            "Word tokenisation report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.94      0.86       894\n",
            "         1.0       0.92      0.75      0.83       887\n",
            "\n",
            "    accuracy                           0.84      1781\n",
            "   macro avg       0.86      0.84      0.84      1781\n",
            "weighted avg       0.86      0.84      0.84      1781\n",
            "\n",
            "-------------------------------------------\n",
            "Alpha: 0.5\n",
            "BPE tokenisation report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.96      0.87       894\n",
            "         1.0       0.94      0.75      0.83       887\n",
            "\n",
            "    accuracy                           0.85      1781\n",
            "   macro avg       0.87      0.85      0.85      1781\n",
            "weighted avg       0.87      0.85      0.85      1781\n",
            "\n",
            "Word tokenisation report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.95      0.86       894\n",
            "         1.0       0.94      0.75      0.83       887\n",
            "\n",
            "    accuracy                           0.85      1781\n",
            "   macro avg       0.87      0.85      0.85      1781\n",
            "weighted avg       0.86      0.85      0.85      1781\n",
            "\n",
            "-------------------------------------------\n",
            "Alpha: 1.0\n",
            "BPE tokenisation report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.96      0.87       894\n",
            "         1.0       0.94      0.75      0.83       887\n",
            "\n",
            "    accuracy                           0.85      1781\n",
            "   macro avg       0.87      0.85      0.85      1781\n",
            "weighted avg       0.87      0.85      0.85      1781\n",
            "\n",
            "Word tokenisation report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.95      0.86       894\n",
            "         1.0       0.94      0.74      0.83       887\n",
            "\n",
            "    accuracy                           0.85      1781\n",
            "   macro avg       0.86      0.85      0.85      1781\n",
            "weighted avg       0.86      0.85      0.85      1781\n",
            "\n",
            "-------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "alpha_values = [0.1, 0.5, 1.0]\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    # Train classifier on both dataframes\n",
        "    bpe_prior, bpe_likelihood = train_nb(X_train_bpe, y_train, alpha=alpha)\n",
        "    word_prior, word_likelihood = train_nb(X_train_word, y_train, alpha=alpha)\n",
        "\n",
        "    # Store predictions in dict\n",
        "    pred_bpe = []\n",
        "    pred_word = []\n",
        "    for i in range(y_dev.shape[0]):\n",
        "        sentiment_bpe = predict_nb(X_dev_bpe[i], bpe_prior, bpe_likelihood)\n",
        "        sentiment_word = predict_nb(X_dev_word[i], word_prior, word_likelihood)\n",
        "        \n",
        "        pred_bpe.append(sentiment_bpe)\n",
        "        pred_word.append(sentiment_word)\n",
        "\n",
        "    # Classification report\n",
        "    print(f'Alpha: {alpha}')\n",
        "    print('BPE tokenisation report:\\n', classification_report(y_dev, pred_bpe))\n",
        "    print('Word tokenisation report:\\n', classification_report(y_dev, pred_word))\n",
        "    print('-------------------------------------------')\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
